{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document embedding (doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea here is to train a neural network model to create text embeddings.\n",
    "Text embedding vectors can then be used to obtrain similarity metrics on neighbouring texts.\n",
    "\n",
    "In this example we use the `doc2vec` model \n",
    "([Le & Mikolov, ICML 2014](https://cs.stanford.edu/~quocle/paragraph_vector.pdf))\n",
    "to create text embeddings using pubmed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ik18445/.local/share/virtualenvs/jgi-data-week-workshop/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec -> doc2vec\n",
    "  - skip-gram -> distributed memory\n",
    "  - continuous-bag-of-words -> distributed bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distributed Memory model (PV-DM)](https://adriancolyer.files.wordpress.com/2016/05/paragraph-vectors-fig-2.png?w=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distributed Bag of Words model (PV-DBW)](https://adriancolyer.files.wordpress.com/2016/05/paragraph-vectors-fig-3.png?w=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pubmed data from earlier sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25475436</td>\n",
       "      <td>2015</td>\n",
       "      <td>Sixty-five common genetic variants and predict...</td>\n",
       "      <td>We developed a 65 type 2 diabetes (T2D) varian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25011450</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association between alcohol and cardiovascular...</td>\n",
       "      <td>To use the rs1229984 variant in the alcohol de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28968714</td>\n",
       "      <td>2018</td>\n",
       "      <td>FATHMM-XF: accurate prediction of pathogenic p...</td>\n",
       "      <td>We present FATHMM-XF, a method for predicting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21965548</td>\n",
       "      <td>2012</td>\n",
       "      <td>Four genetic loci influencing electrocardiogra...</td>\n",
       "      <td>Presence of left ventricular hypertrophy on an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26930047</td>\n",
       "      <td>2016</td>\n",
       "      <td>Diagnosis of Coronary Heart Diseases Using Gen...</td>\n",
       "      <td>Cardiovascular disease (including coronary art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid  year                                              title  \\\n",
       "0  25475436  2015  Sixty-five common genetic variants and predict...   \n",
       "1  25011450  2014  Association between alcohol and cardiovascular...   \n",
       "2  28968714  2018  FATHMM-XF: accurate prediction of pathogenic p...   \n",
       "3  21965548  2012  Four genetic loci influencing electrocardiogra...   \n",
       "4  26930047  2016  Diagnosis of Coronary Heart Diseases Using Gen...   \n",
       "\n",
       "                                            abstract  \n",
       "0  We developed a 65 type 2 diabetes (T2D) varian...  \n",
       "1  To use the rs1229984 variant in the alcohol de...  \n",
       "2  We present FATHMM-XF, a method for predicting ...  \n",
       "3  Presence of left ventricular hypertrophy on an...  \n",
       "4  Cardiovascular disease (including coronary art...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_data = pd.read_csv(config.demoPubmed, sep=\"\\t\")\n",
    "\n",
    "pubmed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the pre-processing steps to be done (subjective):\n",
    "\n",
    "- Make sure `abstract`s are strings.\n",
    "- Keep only abstracts that are sufficiently long enough, and not just fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_enough(text, word_length=40, num_sentences=2):\n",
    "    return (len(text.split(\" \")) >= word_length \n",
    "            and text.count(\".\") >= num_sentences)\n",
    "\n",
    "pubmed_data = pubmed_data \\\n",
    "    .assign(abstract=lambda df: df.abstract.astype(str)) \\\n",
    "    .assign(keep=lambda df: df.abstract.apply(long_enough))\n",
    "\n",
    "pubmed_data_keep = pubmed_data.query(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the rs1229984 variant in the alcohol dehydrogenase 1B gene (ADH1B) as an instrument to investigate the causal role of alcohol in cardiovascular disease. \n",
      "\n",
      "Conclusions. A candidate functional variant, rs28451064, was identified. Future work should focus on identifying the pathway(s) involved. \n",
      "\n",
      "Haptoglobin acts as an antioxidant by limiting peroxidative tissue damage by free hemoglobin. The haptoglobin gene allele Hp2 comprises a 1.7â€‰kb partial duplication. Relative to allele Hp1, Hp2 carriers form protein multimers, suboptimal for hemoglobin scavenging. \n",
      "\n",
      "To establish whether the association between milk intake and prostate cancer operates via the insulin-like growth factor (IGF) pathway (including IGF-I, IGF-II, IGFBP-1, IGFBP-2, and IGFBP-3). \n",
      "\n",
      "Prenatal exposure to maternal cigarette smoking (prenatal smoke exposure) had been associated with altered DNA methylation (DNAm) at birth. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Abstracts to be discarded from the corpus\n",
    "for text in pubmed_data.query(\"not keep\").abstract[:5]:\n",
    "    print(text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two sets of text corpus, in which a text element consists of both the `title` and the `abstract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts in training set: 8379\n",
      "Number of texts in test set: 441\n"
     ]
    }
   ],
   "source": [
    "split = 0.95\n",
    "split_idx = int(np.floor(pubmed_data_keep.shape[0] * split))\n",
    "\n",
    "pubmed_train = pubmed_data_keep[:split_idx]\n",
    "pubmed_test = pubmed_data_keep[split_idx:]\n",
    "train_corpus = []\n",
    "test_corpus = []\n",
    "\n",
    "for i, (title, abstract) in enumerate(zip(pubmed_train.title, \n",
    "                                          pubmed_train.abstract)):\n",
    "    train_corpus.append(TaggedDocument(title + \" \" + abstract, [i]))\n",
    "    \n",
    "for i, (title, abstract) in enumerate(zip(pubmed_test.title,\n",
    "                                          pubmed_test.abstract)):\n",
    "    test_corpus.append(TaggedDocument(title + \" \" + abstract, [i]))\n",
    "    \n",
    "print(f\"Number of texts in training set: {len(train_corpus)}\")\n",
    "print(f\"Number of texts in test set: {len(test_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(Sixty-five common genetic variants and prediction of type 2 diabetes. We developed a 65 type 2 diabetes (T2D) variant-weighted gene score to examine the impact on T2D risk assessment in a U.K.-based consortium of prospective studies, with subjects initially free from T2D (N = 13,294; 37.3% women; mean age 58.5 [38-99] years). We compared the performance of the gene score with the phenotypically derived Framingham Offspring Study T2D risk model and then the two in combination. Over the median 10 years of follow-up, 804 participants developed T2D. The odds ratio for T2D (top vs. bottom quintiles of gene score) was 2.70 (95% CI 2.12-3.43). With a 10% false-positive rate, the genetic score alone detected 19.9% incident cases, the Framingham risk model 30.7%, and together 37.3%. The respective area under the receiver operator characteristic curves were 0.60 (95% CI 0.58-0.62), 0.75 (95% CI 0.73 to 0.77), and 0.76 (95% CI 0.75 to 0.78). The combined risk score net reclassification improvement (NRI) was 8.1% (5.0 to 11.2; P = 3.31 Ã— 10(-7)). While BMI stratification into tertiles influenced the NRI (BMI â‰¤24.5 kg/m(2), 27.6% [95% CI 17.7-37.5], P = 4.82 Ã— 10(-8); 24.5-27.5 kg/m(2), 11.6% [95% CI 5.8-17.4], P = 9.88 Ã— 10(-5); >27.5 kg/m(2), 2.6% [95% CI -1.4 to 6.6], P = 0.20), age categories did not. The addition of the gene score to a phenotypic risk model leads to a potentially clinically important improvement in discrimination of incident T2D. , [0]) \n",
      "\n",
      "TaggedDocument(FATHMM-XF: accurate prediction of pathogenic point mutations via extended features. We present FATHMM-XF, a method for predicting pathogenic point mutations in the human genome. Drawing on an extensive feature set, FATHMM-XF outperforms competitors on benchmark tests, particularly in non-coding regions where the majority of pathogenic mutations are likely to be found., [1]) \n",
      "\n",
      "TaggedDocument(Four genetic loci influencing electrocardiographic indices of left ventricular hypertrophy. Presence of left ventricular hypertrophy on an ECG (ECG-LVH) is widely assessed clinically and provides prognostic information in some settings. There is evidence for significant heritability of ECG-LVH. We conducted a large-scale gene-centric association analysis of 4 commonly measured indices of ECG-LVH., [2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(train_corpus[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Doc2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate a simple usage of paragraph / sentence embedding using a Doc2Vec model.\n",
    "\n",
    "Refer to [gensim's documentation](https://radimrehurek.com/gensim/models/doc2vec.html) \n",
    "on the specific usage of Doc2Vec model and its APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 54s, sys: 6.57 s, total: 3min\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(train_corpus)\n",
    "\n",
    "%time d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdm_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdbow_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdm_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdm_tag_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdocvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdocvecs_mapfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Class for training, using and evaluating neural networks described in http://arxiv.org/pdf/1405.4053v2.pdf\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Initialize the model from an iterable of `documents`. Each document is a\n",
       "TaggedDocument object that will be used for training.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "documents : iterable of iterables\n",
       "    The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
       "    consider an iterable that streams the documents directly from disk/network.\n",
       "    If you don't supply `documents`, the model is left uninitialized -- use if\n",
       "    you plan to initialize it in some other way.\n",
       "\n",
       "dm : int {1,0}\n",
       "    Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
       "    Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
       "\n",
       "size : int\n",
       "    Dimensionality of the feature vectors.\n",
       "window : int\n",
       "    The maximum distance between the current and predicted word within a sentence.\n",
       "alpha : float\n",
       "    The initial learning rate.\n",
       "min_alpha : float\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "min_count : int\n",
       "    Ignores all words with total frequency lower than this.\n",
       "max_vocab_size : int\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "sample : float\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "workers : int\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "iter : int\n",
       "    Number of iterations (epochs) over the corpus.\n",
       "hs : int {1,0}\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
       "negative : int\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If set to 0, no negative sampling is used.\n",
       "dm_mean : int {1,0}\n",
       "    If 0 , use the sum of the context word vectors. If 1, use the mean.\n",
       "    Only applies when `dm` is used in non-concatenative mode.\n",
       "dm_concat : int {1,0}\n",
       "    If 1, use concatenation of context vectors rather than sum/average;\n",
       "    Note concatenation results in a much-larger model, as the input\n",
       "    is no longer the size of one (sampled or arithmetically combined) word vector, but the\n",
       "    size of the tag(s) and all words in the context strung together.\n",
       "dm_tag_count : int\n",
       "    Expected constant number of document tags per document, when using\n",
       "    dm_concat mode; default is 1.\n",
       "dbow_words : int {1,0}\n",
       "    If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\n",
       "    doc-vector training; If 0, only trains doc-vectors (faster).\n",
       "trim_rule : function\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
       "    of the model.\n",
       "callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
       "    List of callbacks that need to be executed/run at specific stages during training.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/jgi-data-week-workshop/lib/python3.6/site-packages/gensim/models/doc2vec.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is how you can check the API usage.\n",
    "# You can do this in the notebook kernel session or \n",
    "# in a jupyter lab console session associated to this kernel\n",
    "\n",
    "?Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check of our model fit,\n",
    "we take the first 1000 abstract from the entire train corpus,\n",
    "and see whether for the given text, the most similar abstract is itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = 1000\n",
    "sample_corpus = train_corpus[:sample_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "for doc_id in range(len(sample_corpus)):\n",
    "    # The inferred_vector of this document\n",
    "    inferred_vector = d2v_model.infer_vector(sample_corpus[doc_id].words)\n",
    "    # Get the most similar document rankings across entire `train_corpus`\n",
    "    # in the form of\n",
    "    # [(499, 0.7327660322189331),\n",
    "    #  (8835, 0.6161227822303772),\n",
    "    #  (981, 0.5684570074081421),\n",
    "    #  ...]\n",
    "    sims = d2v_model.docvecs.most_similar([inferred_vector], \n",
    "                                          topn=len(d2v_model.docvecs))\n",
    "    # The index position for `doc_id`.\n",
    "    # If this abstract is most similar to itself, rank should be 0\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    \n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 995, 1: 2, 2: 2, 3: 1})\n",
      "Accuracy: 99.5%\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(ranks))\n",
    "\n",
    "accuracy = np.sum(np.array(ranks) == 0) / sample_n * 100\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation engine based on abstract embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose documents from `test_corpus` are from authors that are interested in finding collaborators for future works.\n",
    "\n",
    "For a small sample of `test_corpus` documents, we test to find the most similar documents from `train_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(Adaptation and validation of antibody-ELISA using dried blood spots on filter paper for epidemiological surveys of tsetse-transmitted trypanosomosis in cattle. The indirect enzyme-linked immunosorbent assay (ELISA) for the detection of anti-trypanosomal antibodies in bovine serum was adapted for use with dried blood spots on filter paper. Absorbance (450 nm) results for samples were expressed as percent positivity, i.e. percentage of the median absorbance result of four replicates of the strong positive control serum. The antibody-ELISA was evaluated in Zambia for use in epidemiological surveys of the prevalence of tsetse-transmitted bovine trypanosomosis. Known negative samples (sera, n = 209; blood spots, n = 466) were obtained from cattle from closed herds in tsetse-free areas close to Lusaka. Known positive samples (sera, n = 367; blood spots, n = 278) were obtained from cattle in Zambia's Central, Lusaka and Eastern Provinces, diagnosed as being infected with Trypanosoma brucei, T. congolense, or T. vivax using the phase-contrast buffy-coat technique or Giemsa-stained thick and thin blood smears. For sera (at a cut-off value of 23.0% positivity) sensitivity and specificity were 86.1 and 95.2%, respectively. For bloodspots (at a cut-off value of 18.8% positivity) sensitivity and specificity were 96.8 and 95.7%, respectively. The implications of persistence of antibodies following treatment or self-cure are discussed., [0]) \n",
      "\n",
      "TaggedDocument(How human brucellosis incidence in urban Kampala can be reduced most efficiently? A stochastic risk assessment of informally-marketed milk. In Kampala, Uganda, studies have shown a significant incidence of human brucellosis. A stochastic risk assessment involving two field surveys (cattle farms and milk shops) and a medical record survey was conducted to assess the risk of human brucellosis infection through consumption of informally marketed raw milk potentially infected with Brucella abortus in Kampala and to identify the best control options., [1]) \n",
      "\n",
      "TaggedDocument(Herd prevalence of bovine brucellosis and analysis of risk factors in cattle in urban and peri-urban areas of the Kampala economic zone, Uganda. Human brucellosis has been found to be prevalent in the urban areas of Kampala, the capital city of Uganda. A cross-sectional study was designed to generate precise information on the prevalence of brucellosis in cattle and risk factors for the disease in its urban and peri-urban dairy farming systems., [2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_id in range(3):\n",
    "    print(test_corpus[doc_id], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_similar_n = 3\n",
    "similar = []\n",
    "\n",
    "for test_doc_id in range(len(test_corpus)):\n",
    "    words = test_corpus[test_doc_id].words\n",
    "    inferred_vector = d2v_model.infer_vector(words, steps=20)\n",
    "    top_sim = d2v_model.docvecs.most_similar(\n",
    "        [inferred_vector], topn = top_similar_n)\n",
    "    similar.append(top_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank test abstract by similar scores\n",
    "most_similar = []\n",
    "for test_doc_id in range(len(test_corpus)):\n",
    "    item = {\n",
    "        \"test_doc_id\": test_doc_id,\n",
    "        \"train_doc_id\": similar[test_doc_id][0][0],\n",
    "        \"score\": similar[test_doc_id][0][1],\n",
    "    }\n",
    "    most_similar.append(item)\n",
    "    \n",
    "most_similar = pd.DataFrame(most_similar).sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select top 2 test abstracts that have the best match from `train_corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Abstract 256:\n",
      "content: The Rho GTPases in macrophage motility and chemotaxis. The GTP-binding proteins, Rho, Rac and Cdc42 are known to regulate actin organisation. Rho induces the assembly of contractile actin-based microfilaments such as stress fibres, Rac regulates the formation of membrane ruffles and lamellipodia, and Cdc42 activation is necessary for the formation of filopodia. In addition, all three proteins can also regulate the assembly of integrin-containing focal adhesion complexes. The orchestration of these distinct cytoskeletal changes is thought to form the basis of the coordination of cell motility and we have investigated the roles of Rho family proteins in migration using a model system. We have found that in the macrophage cell line Bac1, the cytokine CSF-1 rapidly induces actin reorganisation: it stimulates the formation of filopodia, lamellipodia and membrane ruffles, as well as the appearance of fine actin cables within the cell. We have shown that Cdc42, Rac and Rho regulate the CSF-1 induced formation of these distinct actin filament-based structures. Using a cell tracking procedure we found that both Rho and Rac were required for CSF-1 stimulated cell translocation. In contrast, inhibition of Cdc42 does not prevent macrophages migrating in response to CSF-1, but does prevent recognition of a CSF-1 concentration gradient, so that cells now migrate randomly rather than up the gradient of this chemotactic cytokine. This implies that Cdc42, and thus probably filopodia, are required for gradient sensing and cell polarisation in macrophages.\n",
      "\n",
      "\n",
      "\t ## Matched abstract 0: id 7012, similarity 0.6871333122253418\n",
      "\t Coronin-1C Protein and Caveolin Protein Provide Constitutive and Inducible Mechanisms of Rac1 Protein Trafficking. Sustained directional fibroblast migration requires both polarized activation of the protrusive signal, Rac1, and redistribution of inactive Rac1 from the rear of the cell so that it can be redistributed or degraded. In this work, we determine how alternative endocytic mechanisms dictate the fate of Rac1 in response to the extracellular matrix environment. We discover that both coronin-1C and caveolin retrieve Rac1 from similar locations at the rear and sides of the cell. We find that coronin-1C-mediated extraction, which is responsible for Rac1 recycling, is a constitutive process that maintains Rac1 protein levels within the cell. In the absence of coronin-1C, the effect of caveolin-mediated endocytosis, which targets Rac1 for proteasomal degradation, becomes apparent. Unlike constitutive coronin-1C-mediated trafficking, caveolin-mediated Rac1 endocytosis is induced by engagement of the fibronectin receptor syndecan-4. Such an inducible endocytic/degradation mechanism would predict that, in the presence of fibronectin, caveolin defines regions of the cell that are resistant to Rac1 activation but, in the absence of fibronectin leaves more of the membrane susceptible to Rac1 activation and protrusion. Indeed, we demonstrate that fibronectin-stimulated activation of Rac1 is accelerated in the absence of caveolin and that, when caveolin is knocked down, polarization of active Rac1 is lost in FRET experiments and culminates in shunting migration in a fibrous fibronectin matrix. Although the concept of polarized Rac1 activity in response to chemoattractants has always been apparent, our understanding of the balance between recycling and degradation explains how polarity can be maintained when the chemotactic gradient has faded. \n",
      "\n",
      "\n",
      "\t ## Matched abstract 1: id 7011, similarity 0.6148437857627869\n",
      "\t Integration of the Rac1- and actin-binding properties of Coronin-1C. The coronin family of actin-binding proteins regulate actin branching by inhibiting Arp2/3. We recently reported 2 interactions that were unique to coronin-1C: binding of a Rac1 inhibitor, RCC2, to the unique linker region and Rac1 itself to the propeller domain in a manner that differs from that proposed for other coronins. Through these interactions coronin-1C redistributes Rac1 from the back of the cell to the leading edge for either activation or sequestration by the associated Rac1-inhibitor, RCC2. Here we investigate the relationship between the Rac1- and actin-binding properties of coronin-1C and find that, although actin appears to be involved in the retrafficking of Rac1, signaling by Rac1 lies upstream of the stress fiber-formation, for which the coronins were originally characterized. \n",
      "\n",
      "\n",
      "\t ## Matched abstract 2: id 3855, similarity 0.6147719621658325\n",
      "\t Comparison of the Proteome of Adult and Cord Erythroid Cells, and Changes in the Proteome Following Reticulocyte Maturation. Cord blood stem cells are an attractive starting source for the production of red blood cells in vitro for therapy because of additional expansion potential compared with adult peripheral blood progenitors and cord blood banks usually being more representative of national populations than blood donors. Consequently, it is important to establish how similar cord RBCs are to adult cells. In this study, we used multiplex tandem mass tag labeling combined with nano-LC-MS/MS to compare the proteome of adult and cord RBCs and reticulocytes. 2838 unique proteins were identified, providing the most comprehensive compendium of RBC proteins to date. Using stringent criteria, 1674 proteins were quantified, and only a small number differed in amount between adult and cord RBC. We focused on proteins critical for RBC function. Of these, only the expected differences in globin subunits, along with higher levels of carbonic anhydrase 1 and 2 and aquaporin-1 in adult RBCs would be expected to have a phenotypic effect since they are associated with the differences in gaseous exchange between adults and neonates. Since the RBC and reticulocyte samples used were autologous, we catalogue the change in proteome following reticulocyte maturation. The majority of proteins (>60% of the 1671 quantified) reduced in abundance between 2- and 100-fold following maturation. However, âˆ¼5% were at a higher level in RBCs, localized almost exclusively to cell membranes, in keeping with the known clearance of intracellular recycling pools during reticulocyte maturation. Overall, these data suggest that, with respect to the proteome, there is no barrier to the use of cord progenitors for the in vitro generation of RBCs for transfusion to adults other than the expression of fetal, not adult, hemoglobin.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Abstract 154:\n",
      "content: Noise creates polarization artefacts. The accuracy of calculations of both the degree and angle of polarization depend strongly on the noise in the measurements used. The noise in the measurements recorded by both camera based systems and spectrometers can lead to significant artefacts and incorrect conclusions about high degrees of polarization when in fact none exist. Three approaches are taken in this work: firstly, the absolute error introduced as a function of the signal to noise ratio for polarization measurements is quantified in detail. An important finding here is the reason for why several studies incorrectly suggest that black (low reflectivity) objects are highly polarized. The high degree of polarization is only an artefact of the noise in the calculation. Secondly, several simple steps to avoid such errors are suggested. Thirdly, if these points can not be followed, two methods are presented for mitigating the effects of noise: a maximum likelihood estimation method and a new denoising algorithm to best calculate the degree of polarization of natural polarization information.\n",
      "\n",
      "\n",
      "\t ## Matched abstract 0: id 6335, similarity 0.6815904378890991\n",
      "\t Cerebellar cortical organization: a one-map hypothesis. The fundamental architecture of the cerebellum is concealed within a terminological forest - transverse zones and stripes, longitudinal zones and microzones, patches, etc. To make things worse, the same term is used in different contexts to describe quite different patterns of spatial localization. Here we consider the possibility that this complexity hides the fact that the cerebellar cortex contains only one map, which has been charted in various ways.\n",
      "\n",
      "\n",
      "\t ## Matched abstract 1: id 2874, similarity 0.6477985382080078\n",
      "\t SpaceScanner: COPASI wrapper for automated management of global stochastic optimization experiments. Due to their universal applicability, global stochastic optimization methods are popular for designing improvements of biochemical networks. The drawbacks of global stochastic optimization methods are: (i) no guarantee of finding global optima, (ii) no clear optimization run termination criteria and (iii) no criteria to detect stagnation of an optimization run. The impact of these drawbacks can be partly compensated by manual work that becomes inefficient when the solution space is large due to combinatorial explosion of adjustable parameters or for other reasons.\n",
      "\n",
      "\n",
      "\t ## Matched abstract 2: id 2207, similarity 0.612804651260376\n",
      "\t Dynamic polarization vision in mantis shrimps. Gaze stabilization is an almost ubiquitous animal behaviour, one that is required to see the world clearly and without blur. Stomatopods, however, only fix their eyes on scenes or objects of interest occasionally. Almost uniquely among animals they explore their visual environment with a series pitch, yaw and torsional (roll) rotations of their eyes, where each eye may also move largely independently of the other. In this work, we demonstrate that the torsional rotations are used to actively enhance their ability to see the polarization of light. Both Gonodactylus smithii and Odontodactylus scyllarus rotate their eyes to align particular photoreceptors relative to the angle of polarization of a linearly polarized visual stimulus, thereby maximizing the polarization contrast between an object of interest and its background. This is the first documented example of any animal displaying dynamic polarization vision, in which the polarization information is actively maximized through rotational eye movements.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n = 2\n",
    "preview_n = 3\n",
    "\n",
    "for test_doc_id in most_similar.test_doc_id[:top_n]:\n",
    "    print(f\"# Abstract {test_doc_id}:\")\n",
    "    words = test_corpus[test_doc_id].words\n",
    "    print(f\"content: {words}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    for i in range(preview_n):\n",
    "        train_doc_id = similar[test_doc_id][i][0]\n",
    "        similarity_score = similar[test_doc_id][i][1]\n",
    "        print(f\"\\t ## Matched abstract {i}: id {train_doc_id}, similarity {similarity_score}\")\n",
    "        words = train_corpus[train_doc_id].words\n",
    "        print(f\"\\t {words}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeling lucky with some random text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Early Cannabis Use, Polygenic Risk Score for Schizophrenia and Brain Maturation in Adolescence. Cannabis use during adolescence is known to increase the risk for schizophrenia in men. Sex differences in the dynamics of brain maturation during adolescence may be of particular importance with regard to vulnerability of the male brain to cannabis exposure.',\n",
       " 0.496995210647583)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Searching for the causal effects of body mass index in over 300 000 participants in UK Biobank, using Mendelian randomization.\"\n",
    "\n",
    "def feeling_lucky(text, model=d2v_model, train_corpus=train_corpus):\n",
    "    inferred_vector = model.infer_vector(text, steps=20)\n",
    "    top_sim = model.docvecs.most_similar(\n",
    "        [inferred_vector], topn = 1)\n",
    "    matched_abstract = train_corpus[top_sim[0][0]].words\n",
    "    similarity_score = top_sim[0][1]\n",
    "    return matched_abstract, similarity_score\n",
    "\n",
    "feeling_lucky(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a very crude demo (small training set and lack of pre-processing and model tuning).\n",
    "\n",
    "Below is the overall distribution of top similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>test_doc_id</th>\n",
       "      <th>train_doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.687133</td>\n",
       "      <td>256</td>\n",
       "      <td>7012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.681590</td>\n",
       "      <td>154</td>\n",
       "      <td>6335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.679621</td>\n",
       "      <td>138</td>\n",
       "      <td>4084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.678780</td>\n",
       "      <td>260</td>\n",
       "      <td>2168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.676889</td>\n",
       "      <td>300</td>\n",
       "      <td>8202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.671718</td>\n",
       "      <td>403</td>\n",
       "      <td>5784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.665003</td>\n",
       "      <td>293</td>\n",
       "      <td>8185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.661996</td>\n",
       "      <td>368</td>\n",
       "      <td>5598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.656170</td>\n",
       "      <td>440</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.653303</td>\n",
       "      <td>286</td>\n",
       "      <td>8185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.652377</td>\n",
       "      <td>315</td>\n",
       "      <td>8185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.652225</td>\n",
       "      <td>389</td>\n",
       "      <td>4430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.642343</td>\n",
       "      <td>407</td>\n",
       "      <td>5878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.641203</td>\n",
       "      <td>354</td>\n",
       "      <td>5598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.638505</td>\n",
       "      <td>310</td>\n",
       "      <td>8185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.636913</td>\n",
       "      <td>360</td>\n",
       "      <td>4872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.635799</td>\n",
       "      <td>401</td>\n",
       "      <td>5598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.628839</td>\n",
       "      <td>103</td>\n",
       "      <td>2687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.625694</td>\n",
       "      <td>350</td>\n",
       "      <td>5370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.624060</td>\n",
       "      <td>272</td>\n",
       "      <td>5743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.620159</td>\n",
       "      <td>282</td>\n",
       "      <td>7352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.619087</td>\n",
       "      <td>198</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.618067</td>\n",
       "      <td>423</td>\n",
       "      <td>6234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.614517</td>\n",
       "      <td>357</td>\n",
       "      <td>5784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.613666</td>\n",
       "      <td>247</td>\n",
       "      <td>5250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.612984</td>\n",
       "      <td>413</td>\n",
       "      <td>8185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.612803</td>\n",
       "      <td>153</td>\n",
       "      <td>5161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.612589</td>\n",
       "      <td>344</td>\n",
       "      <td>7012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.611871</td>\n",
       "      <td>246</td>\n",
       "      <td>5118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.611774</td>\n",
       "      <td>82</td>\n",
       "      <td>3665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.444359</td>\n",
       "      <td>9</td>\n",
       "      <td>3852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.441462</td>\n",
       "      <td>325</td>\n",
       "      <td>6890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.441223</td>\n",
       "      <td>66</td>\n",
       "      <td>4844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.441026</td>\n",
       "      <td>430</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.440331</td>\n",
       "      <td>86</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.439796</td>\n",
       "      <td>212</td>\n",
       "      <td>8219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.437693</td>\n",
       "      <td>313</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.435546</td>\n",
       "      <td>12</td>\n",
       "      <td>7238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.433955</td>\n",
       "      <td>324</td>\n",
       "      <td>7064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.432634</td>\n",
       "      <td>363</td>\n",
       "      <td>4739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.432582</td>\n",
       "      <td>209</td>\n",
       "      <td>1533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.432152</td>\n",
       "      <td>119</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.432091</td>\n",
       "      <td>177</td>\n",
       "      <td>1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.430848</td>\n",
       "      <td>167</td>\n",
       "      <td>2945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.429054</td>\n",
       "      <td>4</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.428183</td>\n",
       "      <td>102</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.427085</td>\n",
       "      <td>420</td>\n",
       "      <td>8158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.426860</td>\n",
       "      <td>419</td>\n",
       "      <td>2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.423371</td>\n",
       "      <td>19</td>\n",
       "      <td>5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.422620</td>\n",
       "      <td>380</td>\n",
       "      <td>6187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.421899</td>\n",
       "      <td>108</td>\n",
       "      <td>3096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.421886</td>\n",
       "      <td>298</td>\n",
       "      <td>6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.421874</td>\n",
       "      <td>250</td>\n",
       "      <td>2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.420970</td>\n",
       "      <td>84</td>\n",
       "      <td>3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.418832</td>\n",
       "      <td>230</td>\n",
       "      <td>1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.412839</td>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.405065</td>\n",
       "      <td>432</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.403615</td>\n",
       "      <td>224</td>\n",
       "      <td>5862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.402851</td>\n",
       "      <td>348</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.387544</td>\n",
       "      <td>437</td>\n",
       "      <td>5425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  test_doc_id  train_doc_id\n",
       "256  0.687133          256          7012\n",
       "154  0.681590          154          6335\n",
       "138  0.679621          138          4084\n",
       "260  0.678780          260          2168\n",
       "300  0.676889          300          8202\n",
       "403  0.671718          403          5784\n",
       "293  0.665003          293          8185\n",
       "368  0.661996          368          5598\n",
       "440  0.656170          440          1988\n",
       "286  0.653303          286          8185\n",
       "315  0.652377          315          8185\n",
       "389  0.652225          389          4430\n",
       "407  0.642343          407          5878\n",
       "354  0.641203          354          5598\n",
       "310  0.638505          310          8185\n",
       "360  0.636913          360          4872\n",
       "401  0.635799          401          5598\n",
       "103  0.628839          103          2687\n",
       "350  0.625694          350          5370\n",
       "272  0.624060          272          5743\n",
       "282  0.620159          282          7352\n",
       "198  0.619087          198           358\n",
       "423  0.618067          423          6234\n",
       "357  0.614517          357          5784\n",
       "247  0.613666          247          5250\n",
       "413  0.612984          413          8185\n",
       "153  0.612803          153          5161\n",
       "344  0.612589          344          7012\n",
       "246  0.611871          246          5118\n",
       "82   0.611774           82          3665\n",
       "..        ...          ...           ...\n",
       "9    0.444359            9          3852\n",
       "325  0.441462          325          6890\n",
       "66   0.441223           66          4844\n",
       "430  0.441026          430           227\n",
       "86   0.440331           86           727\n",
       "212  0.439796          212          8219\n",
       "313  0.437693          313           338\n",
       "12   0.435546           12          7238\n",
       "324  0.433955          324          7064\n",
       "363  0.432634          363          4739\n",
       "209  0.432582          209          1533\n",
       "119  0.432152          119          1951\n",
       "177  0.432091          177          1155\n",
       "167  0.430848          167          2945\n",
       "4    0.429054            4           396\n",
       "102  0.428183          102           773\n",
       "420  0.427085          420          8158\n",
       "419  0.426860          419          2423\n",
       "19   0.423371           19          5037\n",
       "380  0.422620          380          6187\n",
       "108  0.421899          108          3096\n",
       "298  0.421886          298          6961\n",
       "250  0.421874          250          2894\n",
       "84   0.420970           84          3125\n",
       "230  0.418832          230          1464\n",
       "0    0.412839            0           425\n",
       "432  0.405065          432          1235\n",
       "224  0.403615          224          5862\n",
       "348  0.402851          348           261\n",
       "437  0.387544          437          5425\n",
       "\n",
       "[441 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [introduction to doc2vec](https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/)\n",
    "- [sentiment analysis](http://linanqiu.github.io/2015/10/07/word2vec-sentiment/)\n",
    "- https://medium.com/@ermolushka/text-clusterization-using-python-and-doc2vec-8c499668fa61\n",
    "- https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
