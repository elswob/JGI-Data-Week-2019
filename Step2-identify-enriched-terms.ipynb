{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "TFIDF (Term Frequency - Inverse Document Frequency) is a statistical method used to quantify the importance of words within a given text, compared to a background corpus.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "https://triton.ml/blog/tf-idf-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF from scratch\n",
    "\n",
    "First, lets read in the complete set of publication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11160 publications\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#read in the preprocessed publication data\n",
    "file = open(config.demoPubmedFile)\n",
    "reader = csv.reader(file,delimiter='\\t')\n",
    "pubData=[]\n",
    "for row in reader:\n",
    "    text=row[2]+' '+row[3]\n",
    "    pubData.append(text.lower().split())\n",
    "    \n",
    "#Removes header\n",
    "pubData = pubData[1:]\n",
    "print(len(pubData),'publications')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing a TF Map \n",
    "\n",
    "(Modified from https://triton.ml/blog/tf-idf-from-scratch)\n",
    "\n",
    "**TF(term) = # of times the term appears in document / total # of terms in document** \n",
    "\n",
    "Now that our data is usable, we’d like to start computing the TF and the IDF. Computing the tf of a word in a publication requires us to calculate the number of words in a publication, and the number of times each word appears in the publication. We can store each (word, word count pair) in a dictionary. The keys of the dictionary are then just the unique terms in the publication. The following function takes in a publication and outputs a tf dictionary for that publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePublicationTFDict(publication):\n",
    "    \"\"\" Returns a tf dictionary for each publication whose keys are all \n",
    "    the unique words in the publication and whose values are their \n",
    "    corresponding tf.\n",
    "    \"\"\"\n",
    "    #Counts the number of times the word appears in publication\n",
    "    publicationTFDict = {}\n",
    "    for word in publication:\n",
    "        if word in publicationTFDict:\n",
    "            publicationTFDict[word] += 1\n",
    "        else:\n",
    "            publicationTFDict[word] = 1\n",
    "    #Computes tf for each word           \n",
    "    for word in publicationTFDict:\n",
    "        publicationTFDict[word] = publicationTFDict[word] / len(publication)\n",
    "    return publicationTFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         term    tf-idf\n",
      "0  sixty-five  0.004184\n",
      "1      common  0.004184\n",
      "2     genetic  0.008368\n",
      "3    variants  0.004184\n",
      "4         and  0.016736\n"
     ]
    }
   ],
   "source": [
    "#run for each list\n",
    "tfDict={}\n",
    "for d in range(0,len(pubData)):\n",
    "    tfDict[d]=computePublicationTFDict(pubData[d])\n",
    "\n",
    "#create df for first publication\n",
    "resDic=[]\n",
    "for t in tfDict[0]:\n",
    "    resDic.append({'term':t,'tf':tfDict[0][t]})\n",
    "df=pd.DataFrame.from_dict(resDic)    \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing an IDF Map\n",
    "\n",
    "(Modified from https://triton.ml/blog/tf-idf-from-scratch)\n",
    "\n",
    "**IDF(term) = log(total # of documents / # of documents with term in it)** \n",
    "\n",
    "Computing the idf of a word requires us to compute the total number of documents and the number of documents that contains the word. In our case we can calculate the total number of documents with len(data), the number of publications. For each publication, we increment the document count for each unique word. We can use the keys of the dictionaries that we calculated in the TF step to get the unique set of words. The resulting IDF dictionary’s keys will be the set of all unique words across every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetic 1001\n"
     ]
    }
   ],
   "source": [
    "def computeCountDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in\n",
    "    the dataset and whose values count the number of reviews in which\n",
    "    the word appears.\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    # Run through each publications's tf dictionary and increment countDict's (word, doc) pair\n",
    "    for review in tfDict:\n",
    "        for word in tfDict[review]:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "\n",
    "#Stores the publication count dictionary\n",
    "countDict = computeCountDict()\n",
    "testWord='genetic'\n",
    "print(testWord,countDict[testWord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute an idfDict, using countDict and some math, and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4113364566200812\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDFDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "        idfDict[word] = math.log(len(pubData) / countDict[word])\n",
    "    return idfDict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "idfDict = computeIDFDict()\n",
    "\n",
    "print(idfDict[\"genetic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there are 11,160 publictions, and the word genetic is mentioned 1,001 times. Therefore the idf = log(11160/1001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the TF-IDF Map \n",
    "\n",
    "(Modified from https://triton.ml/blog/tf-idf-from-scratch)\n",
    "\n",
    "**TF-IDF(term) = TF(term) * IDF(term)**\n",
    "\n",
    "The last step is to compute the TF-IDF. We use our existing tf dictionaries and simply multiply each value by the idf. We can use the idf keys since they contain every unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         term    tf-idf\n",
      "0  sixty-five  0.004184\n",
      "1      common  0.004184\n",
      "2     genetic  0.008368\n",
      "3    variants  0.004184\n",
      "4         and  0.016736\n"
     ]
    }
   ],
   "source": [
    "def computeReviewTFIDFDict(reviewTFDict):\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    review and whose values are their corresponding tfidf.\n",
    "    \"\"\"\n",
    "    reviewTFIDFDict = {}\n",
    "    #For each word in the publication, we multiply its tf and its idf.\n",
    "    for word in tfDict[reviewTFDict]:\n",
    "        reviewTFIDFDict[word] = tfDict[reviewTFDict][word] * idfDict[word]\n",
    "    return reviewTFIDFDict\n",
    "\n",
    "#Stores the TF-IDF dictionaries\n",
    "tfidfDict = [computeReviewTFIDFDict(review) for review in tfDict]\n",
    "#print(tfidfDict[0])\n",
    "\n",
    "#create df for first publication\n",
    "resDic=[]\n",
    "for t in tfDict[0]:\n",
    "    resDic.append({'term':t,'tf-idf':tfDict[0][t]})\n",
    "df=pd.DataFrame.from_dict(resDic)    \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using the **genetic** example before, for publication 1, we had a tf value of **'genetic': 0.008368200836820083** and the idf is 2.4113364566200812. Multiply these together, and you get 0.02 (ish) as seen above :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF using sklearn\n",
    "\n",
    "The above has been implemented in the python package scikit-learn (sklearn) - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html and can be achieved in just a few lines.\n",
    "\n",
    "First let's use some functions to load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orcid_to_pubmed\n",
      "load_orcid\n",
      "load_pubmed\n",
      "Reading corpus\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from scripts.common_functions import orcid_to_pubmed\n",
    "\n",
    "#create dictionary of orcid to publication text\n",
    "orcidToPubmed = orcid_to_pubmed()\n",
    "\n",
    "print('Reading corpus')\n",
    "token_dict = {}\n",
    "for orcid in orcidToPubmed:\n",
    "    token_dict[orcid] = orcidToPubmed[orcid].lower()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the sklean TfidfVectorizer (Equivalent to [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) followed by [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup\n",
      "Transform\n",
      "CPU times: user 14.1 s, sys: 500 ms, total: 14.6 s\n",
      "Wall time: 12.7 s\n",
      "Create matrix\n",
      "Store\n",
      "Stored 'matrix' (ndarray)\n",
      "Stored 'token_dict' (dict)\n",
      "Stored 'tfs' (csr_matrix)\n",
      "Stored 'tfidf' (TfidfVectorizer)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "#setup sklean TfidfVectorizer including bigrams and trigrams\n",
    "print('Setup')\n",
    "tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
    "\n",
    "\n",
    "#fit_transform creates the tf-idf model and returns term-document frequency matrix\n",
    "print('Transform')\n",
    "%time tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "#get similarity matrix for all people\n",
    "#https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "print('Create matrix')\n",
    "matrix=(tfs * tfs.T).A\n",
    "\n",
    "#store this and the dictionary for access in other notebooks\n",
    "print('Store')\n",
    "%store matrix\n",
    "%store token_dict\n",
    "%store tfs\n",
    "%store tfidf\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to test a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t2d', 0.2771374468347099)\n",
      "('gene score', 0.18732077434507402)\n",
      "('95 ci', 0.1680563752219566)\n",
      "('ci', 0.16519742224518033)\n",
      "('95', 0.14483197609614537)\n",
      "('score', 0.14159779049586887)\n",
      "('risk model', 0.12883139598887866)\n",
      "('nri', 0.09366038717253701)\n",
      "('framingham', 0.09366038717253701)\n",
      "('27 kg', 0.09064520376269428)\n",
      "('genetic', 0.03563686144053027)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_doc(tfidf='',text=''):\n",
    "    text=text.lower()\n",
    "    #transform function transforms a document to document-term matrix\n",
    "    response = tfidf.transform([text])\n",
    "\n",
    "    #get the feature name from the model\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    res={}\n",
    "    for col in response.nonzero()[1]:\n",
    "        res[feature_names[col]]=response[0, col]\n",
    "        #reverse sort the results\n",
    "        sorted_res = sorted(res.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return sorted_res\n",
    "\n",
    "#create a test document using the first publication \n",
    "testText=\",\".join(pubData[0])\n",
    "sorted_res=tfidf_doc(tfidf=tfidf,text=testText)\n",
    "for s in sorted_res[:10]:\n",
    "    print(s)\n",
    "\n",
    "#get the genetic score for comparison to above\n",
    "for s in sorted_res:\n",
    "    if s[0] == 'genetic':\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why the different tf-idf scores to the first time?\n",
    "\n",
    "The first example treated each publication separately. In the sklearn example above, each person's collection of publications is treated as a single record. This does mean that there are duplicate publications in the model. Combine this with the slight variation of tf-idf in sklearn, the use of bigrams and trigrams, and the stopwords removal in sklearn, and this might explain the difference. \n",
    "\n",
    "For example, the top term in sklean is **t2d** (type 2 diabetes). However, in the manual tf-idf, method this is split across three results **(t2d)**, **td.** and **td**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF on our data\n",
    "\n",
    "We can now identify the key words in each person's publications, by creating a single document of all texts and comapring to the background frequencies. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_orcid\n",
      "load_pubmed\n",
      "('nematode', 0.1399503126881953)\n",
      "('mammary', 0.1399503126881953)\n",
      "('ccr5', 0.13745566504259402)\n",
      "('motu', 0.11465349697937983)\n",
      "('crpc', 0.10690996169979536)\n",
      "('cell', 0.09380439488333107)\n",
      "('cancer', 0.09324539850690329)\n",
      "('mammary stem', 0.09163711002839603)\n",
      "('id4', 0.09163711002839603)\n",
      "('genome', 0.08541729113595346)\n"
     ]
    }
   ],
   "source": [
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "\n",
    "#get all publications for a specific ORCID\n",
    "orcidID='0000-0001-7328-4233'\n",
    "oText=''\n",
    "for p in orcidToPubmedID[orcidID]:\n",
    "    if p in pubmedText:\n",
    "        oText+=(pubmedText[p])\n",
    "res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "for r in res[0:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily do this for all ORCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=open(config.tfidfFile,'w')\n",
    "counter=0\n",
    "\n",
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    #don't really want to do this for all, so just orcid with < 100 publications!\n",
    "    if len(orcidToPubmedID[orcid])<100:\n",
    "        counter+=1\n",
    "        if counter<=5:\n",
    "            print(counter,orcid)\n",
    "            oText=''\n",
    "            for p in orcidToPubmedID[orcid]:\n",
    "                if p in pubmedText:\n",
    "                    oText+=(pubmedText[p])\n",
    "            print(len(oText))\n",
    "            %time res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "            for r in res[0:100]:\n",
    "                o.write(orcid+'\\t'+r[0]+'\\t'+str(r[1])+'\\n')\n",
    "o.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
