{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "TFIDF (Term Frequency - Inverse Document Frequency) is a statistical method used to quantify the importance of words within a given text, compared to a background corpus.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "https://triton.ml/blog/tf-idf-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF from scratch\n",
    "\n",
    "First, lets read in the complete set of publication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11160 publications\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import csv\n",
    "\n",
    "file = open(config.demoPubmedFile)\n",
    "reader = csv.reader(file,delimiter='\\t')\n",
    "pubData=[]\n",
    "for row in reader:\n",
    "    text=row[2]+' '+row[3]\n",
    "    pubData.append(text.lower().split())\n",
    "    \n",
    "#Removes header\n",
    "pubData = pubData[1:]\n",
    "print(len(pubData),'publications')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing a TF Map\n",
    "\n",
    "**TF(term) = # of times the term appears in document / total # of terms in document** \n",
    "\n",
    "Now that our data is usable, we’d like to start computing the TF and the IDF. Computing the tf of a word in a publication requires us to calculate the number of words in a publication, and the number of times each word appears in the publication. We can store each (word, word count pair) in a dictionary. The keys of the dictionary are then just the unique terms in the publication. The following function takes in a publication and outputs a tf dictionary for that publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePublicationTFDict(publication):\n",
    "    \"\"\" Returns a tf dictionary for each publication whose keys are all \n",
    "    the unique words in the publication and whose values are their \n",
    "    corresponding tf.\n",
    "    \"\"\"\n",
    "    #Counts the number of times the word appears in publication\n",
    "    publicationTFDict = {}\n",
    "    for word in publication:\n",
    "        if word in publicationTFDict:\n",
    "            publicationTFDict[word] += 1\n",
    "        else:\n",
    "            publicationTFDict[word] = 1\n",
    "    #Computes tf for each word           \n",
    "    for word in publicationTFDict:\n",
    "        publicationTFDict[word] = publicationTFDict[word] / len(publication)\n",
    "    return publicationTFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sixty-five': 0.0041841004184100415, 'common': 0.0041841004184100415, 'genetic': 0.008368200836820083, 'variants': 0.0041841004184100415, 'and': 0.016736401673640166, 'prediction': 0.0041841004184100415, 'of': 0.029288702928870293, 'type': 0.008368200836820083, '2': 0.008368200836820083, 'diabetes.': 0.0041841004184100415, 'we': 0.008368200836820083, 'developed': 0.008368200836820083, 'a': 0.02092050209205021, '65': 0.0041841004184100415, 'diabetes': 0.0041841004184100415, '(t2d)': 0.0041841004184100415, 'variant-weighted': 0.0041841004184100415, 'gene': 0.016736401673640166, 'score': 0.02092050209205021, 'to': 0.029288702928870293, 'examine': 0.0041841004184100415, 'the': 0.06276150627615062, 'impact': 0.0041841004184100415, 'on': 0.0041841004184100415, 't2d': 0.016736401673640166, 'risk': 0.02092050209205021, 'assessment': 0.0041841004184100415, 'in': 0.012552301255230125, 'u.k.-based': 0.0041841004184100415, 'consortium': 0.0041841004184100415, 'prospective': 0.0041841004184100415, 'studies,': 0.0041841004184100415, 'with': 0.012552301255230125, 'subjects': 0.0041841004184100415, 'initially': 0.0041841004184100415, 'free': 0.0041841004184100415, 'from': 0.0041841004184100415, '(n': 0.0041841004184100415, '=': 0.02092050209205021, '13,294;': 0.0041841004184100415, '37.3%': 0.0041841004184100415, 'women;': 0.0041841004184100415, 'mean': 0.0041841004184100415, 'age': 0.008368200836820083, '58.5': 0.0041841004184100415, '[38-99]': 0.0041841004184100415, 'years).': 0.0041841004184100415, 'compared': 0.0041841004184100415, 'performance': 0.0041841004184100415, 'phenotypically': 0.0041841004184100415, 'derived': 0.0041841004184100415, 'framingham': 0.008368200836820083, 'offspring': 0.0041841004184100415, 'study': 0.0041841004184100415, 'model': 0.012552301255230125, 'then': 0.0041841004184100415, 'two': 0.0041841004184100415, 'combination.': 0.0041841004184100415, 'over': 0.0041841004184100415, 'median': 0.0041841004184100415, '10': 0.0041841004184100415, 'years': 0.0041841004184100415, 'follow-up,': 0.0041841004184100415, '804': 0.0041841004184100415, 'participants': 0.0041841004184100415, 't2d.': 0.008368200836820083, 'odds': 0.0041841004184100415, 'ratio': 0.0041841004184100415, 'for': 0.0041841004184100415, '(top': 0.0041841004184100415, 'vs.': 0.0041841004184100415, 'bottom': 0.0041841004184100415, 'quintiles': 0.0041841004184100415, 'score)': 0.0041841004184100415, 'was': 0.008368200836820083, '2.70': 0.0041841004184100415, '(95%': 0.016736401673640166, 'ci': 0.029288702928870293, '2.12-3.43).': 0.0041841004184100415, '10%': 0.0041841004184100415, 'false-positive': 0.0041841004184100415, 'rate,': 0.0041841004184100415, 'alone': 0.0041841004184100415, 'detected': 0.0041841004184100415, '19.9%': 0.0041841004184100415, 'incident': 0.008368200836820083, 'cases,': 0.0041841004184100415, '30.7%,': 0.0041841004184100415, 'together': 0.0041841004184100415, '37.3%.': 0.0041841004184100415, 'respective': 0.0041841004184100415, 'area': 0.0041841004184100415, 'under': 0.0041841004184100415, 'receiver': 0.0041841004184100415, 'operator': 0.0041841004184100415, 'characteristic': 0.0041841004184100415, 'curves': 0.0041841004184100415, 'were': 0.0041841004184100415, '0.60': 0.0041841004184100415, '0.58-0.62),': 0.0041841004184100415, '0.75': 0.008368200836820083, '0.73': 0.0041841004184100415, '0.77),': 0.0041841004184100415, '0.76': 0.0041841004184100415, '0.78).': 0.0041841004184100415, 'combined': 0.0041841004184100415, 'net': 0.0041841004184100415, 'reclassification': 0.0041841004184100415, 'improvement': 0.008368200836820083, '(nri)': 0.0041841004184100415, '8.1%': 0.0041841004184100415, '(5.0': 0.0041841004184100415, '11.2;': 0.0041841004184100415, 'p': 0.016736401673640166, '3.31': 0.0041841004184100415, '×': 0.012552301255230125, '10(-7)).': 0.0041841004184100415, 'while': 0.0041841004184100415, 'bmi': 0.0041841004184100415, 'stratification': 0.0041841004184100415, 'into': 0.0041841004184100415, 'tertiles': 0.0041841004184100415, 'influenced': 0.0041841004184100415, 'nri': 0.0041841004184100415, '(bmi': 0.0041841004184100415, '≤24.5': 0.0041841004184100415, 'kg/m(2),': 0.012552301255230125, '27.6%': 0.0041841004184100415, '[95%': 0.012552301255230125, '17.7-37.5],': 0.0041841004184100415, '4.82': 0.0041841004184100415, '10(-8);': 0.0041841004184100415, '24.5-27.5': 0.0041841004184100415, '11.6%': 0.0041841004184100415, '5.8-17.4],': 0.0041841004184100415, '9.88': 0.0041841004184100415, '10(-5);': 0.0041841004184100415, '>27.5': 0.0041841004184100415, '2.6%': 0.0041841004184100415, '-1.4': 0.0041841004184100415, '6.6],': 0.0041841004184100415, '0.20),': 0.0041841004184100415, 'categories': 0.0041841004184100415, 'did': 0.0041841004184100415, 'not.': 0.0041841004184100415, 'addition': 0.0041841004184100415, 'phenotypic': 0.0041841004184100415, 'leads': 0.0041841004184100415, 'potentially': 0.0041841004184100415, 'clinically': 0.0041841004184100415, 'important': 0.0041841004184100415, 'discrimination': 0.0041841004184100415}\n"
     ]
    }
   ],
   "source": [
    "#run for each list\n",
    "tfDict={}\n",
    "for d in range(0,len(pubData)-1):\n",
    "    tfDict[d]=computePublicationTFDict(pubData[d])\n",
    "print(tfDict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing an IDF Map\n",
    "\n",
    "**IDF(term) = log(total # of documents / # of documents with term in it)** \n",
    "\n",
    "Computing the idf of a word requires us to compute the total number of documents and the number of documents that contains the word. In our case we can calculate the total number of documents with len(data), the number of publications. For each publication, we increment the document count for each unique word. We can use the keys of the dictionaries that we calculated in the TF step to get the unique set of words. The resulting IDF dictionary’s keys will be the set of all unique words across every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetic 1001\n"
     ]
    }
   ],
   "source": [
    "def computeCountDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in\n",
    "    the dataset and whose values count the number of reviews in which\n",
    "    the word appears.\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    # Run through each publications's tf dictionary and increment countDict's (word, doc) pair\n",
    "    for review in tfDict:\n",
    "        for word in tfDict[review]:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "\n",
    "#Stores the publication count dictionary\n",
    "countDict = computeCountDict()\n",
    "testWord='genetic'\n",
    "print(testWord,countDict[testWord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute an idfDict, using countDict and some math, and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4113364566200812\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDFDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "        idfDict[word] = math.log(len(pubData) / countDict[word])\n",
    "    return idfDict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "idfDict = computeIDFDict()\n",
    "\n",
    "print(idfDict[\"genetic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the TF-IDF Map\n",
    "\n",
    "**TF-IDF(term) = TF(term) * IDF(term)**\n",
    "\n",
    "The last step is to compute the TF-IDF. We use our existing tf dictionaries and simply multiply each value by the idf. We can use the idf keys since they contain every unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-94f2aff51235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#tfidfDict = [computeReviewTFIDFDict(review) for review in tfDict]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtfidfData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeReviewTFIDFDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestText\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-94f2aff51235>\u001b[0m in \u001b[0;36mcomputeReviewTFIDFDict\u001b[0;34m(reviewTFDict)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#For each word in the publication, we multiply its tf and its idf.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviewTFDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mreviewTFIDFDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviewTFDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midfDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreviewTFIDFDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def computeReviewTFIDFDict(reviewTFDict):\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    review and whose values are their corresponding tfidf.\n",
    "    \"\"\"\n",
    "    reviewTFIDFDict = {}\n",
    "    #For each word in the publication, we multiply its tf and its idf.\n",
    "    for word in reviewTFDict:\n",
    "        reviewTFIDFDict[word] = reviewTFDict[word] * idfDict[word]\n",
    "    return reviewTFIDFDict\n",
    "\n",
    "#Stores the TF-IDF dictionaries\n",
    "testText=\"\"\"\n",
    "'Diagnosis of Coronary Heart Diseases Using Gene Expression Profiling; Stable Coronary Artery Disease, Cardiac Ischemia with and without Myocardial Necrosis.\n",
    "\"\"\"\n",
    "#tfidfDict = [computeReviewTFIDFDict(review) for review in tfDict]\n",
    "tfidfData = computeReviewTFIDFDict([testText])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF using sklearn\n",
    "\n",
    "The above has been implemented in the python package scikit-learn (sklearn) - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "And can be achieved in just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus\n",
      "orcid_to_pubmed\n",
      "load_orcid\n",
      "load_pubmed\n",
      "CPU times: user 14 s, sys: 499 ms, total: 14.5 s\n",
      "Wall time: 12.7 s\n",
      "Stored 'matrix' (ndarray)\n",
      "Stored 'token_dict' (dict)\n",
      "Stored 'tfs' (csr_matrix)\n",
      "Stored 'tfidf' (TfidfVectorizer)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#first lets create some functions to help process the data\n",
    "\n",
    "#load orcid to pmid data\n",
    "def load_orcid():\n",
    "    print('load_orcid')\n",
    "    orcidToPubmedID={}\n",
    "    with open(config.demoOrcidFile) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            orcid,pmid = line.rstrip().split('\\t')\n",
    "            if orcid in orcidToPubmedID:\n",
    "                orcidToPubmedID[orcid].append(pmid)\n",
    "            else:\n",
    "                orcidToPubmedID[orcid]=[pmid]\n",
    "    return orcidToPubmedID\n",
    "\n",
    "#load the publication data\n",
    "def load_pubmed():\n",
    "    print('load_pubmed')\n",
    "    pubmedText={}\n",
    "    with open(config.demoPubmedFile, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            text=row[2]+' '+row[3]\n",
    "            pubmedText[row[0]]=text\n",
    "    return pubmedText\n",
    "\n",
    "#create dictionary of orcid to publication text\n",
    "def orcid_to_pubmed():\n",
    "    print('orcid_to_pubmed')\n",
    "    orcidToPubmedID=load_orcid()\n",
    "    pubmedText = load_pubmed()\n",
    "    orcidToPubmed={}\n",
    "    for orcid in orcidToPubmedID:\n",
    "        oText=''\n",
    "        for p in orcidToPubmedID[orcid]:\n",
    "            if p in pubmedText:\n",
    "                oText+=(pubmedText[p])\n",
    "        orcidToPubmed[orcid]=oText\n",
    "    return orcidToPubmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "print('Reading corpus')\n",
    "token_dict = {}\n",
    "orcidToPubmed = orcid_to_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    token_dict[orcid] = orcidToPubmed[orcid].lower()\n",
    "\n",
    "#sklearn using stemming\n",
    "#tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "#sklean including bigrams and trigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
    "\n",
    "#fit_transform creates the tf-idf model and returns term-document frequency matrix\n",
    "%time tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "#get similarity matrix for all people\n",
    "#https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "matrix=(tfs * tfs.T).A\n",
    "#store this and the dictionary for access in other notebooks\n",
    "%store matrix\n",
    "%store token_dict\n",
    "%store tfs\n",
    "%store tfidf\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to test some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('coronari', 0.5332958702632749)\n",
      "('ischemia', 0.34371259179400093)\n",
      "('necrosi', 0.30789692154467496)\n",
      "('myocardi', 0.3023646678461049)\n",
      "('diseas', 0.2814253031683374)\n",
      "('cardiac', 0.25568154892539324)\n",
      "('arteri', 0.2473787489841284)\n",
      "('heart', 0.20969053733615223)\n",
      "('stabl', 0.2020190573412118)\n",
      "('profil', 0.18987653113237035)\n",
      "('gene', 0.16276434565535075)\n",
      "('express', 0.15482773317357482)\n",
      "(';', 0.1194396689449871)\n",
      "('use', 0.08738171772512363)\n",
      "(',', 0.07874455630641826)\n",
      "('.', 0.07785807398148253)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_doc(tfidf='',text=''):\n",
    "    text=text.lower()\n",
    "    #transform function transforms a document to document-term matrix\n",
    "    response = tfidf.transform([text])\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    res={}\n",
    "    for col in response.nonzero()[1]:\n",
    "        res[feature_names[col]]=response[0, col]\n",
    "        #reverse sort the results\n",
    "        sorted_res = sorted(res.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return sorted_res\n",
    "\n",
    "#sorted_res=tfidf_doc(tfidf=tfidf,text='genetic mendelian the')\n",
    "testText=\"\"\"\n",
    "'Diagnosis of Coronary Heart Diseases Using Gene Expression Profiling; Stable Coronary Artery Disease, Cardiac Ischemia with and without Myocardial Necrosis.\n",
    "\"\"\"\n",
    "sorted_res=tfidf_doc(tfidf=tfidf,text=testText)\n",
    "for s in sorted_res:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why the difference tf-idf values to the first time?\n",
    "\n",
    "The first example was treating each publication separately. In the sklearn example above, each person's collection of publications was treated as a single record. This does mean that there is duplicate publications in the model. Combine this with the slight variation of tf-idf in sklearn, the use of bigrams and trigrams, and this might explain the difference. \n",
    "\n",
    "What we could do is a proper test, comparing the distribution of tf-idf scores across both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of': 0.03371617768312149, 'coronary': 3.6676020556666513, 'heart': 3.160995847443369, 'diseases': 4.036887507197314, 'using': 1.5146166106644452, 'gene': 2.6789090661947106, 'expression': 2.692049859755769, 'profiling;': 9.320091235935301, 'stable': 4.44489391273415, 'artery': 4.44489391273415, 'disease,': 4.062595863907521, 'cardiac': 3.736594927153603, 'ischemia': 5.558891120241739, 'with': 0.41559690703857133, 'and': 0.0612464548995043, 'without': 3.3014980214390675, 'myocardial': 4.46027883157363, 'necrosis.': 7.933796874815411}\n",
      "[('coronary', 0.5174931509088351), ('ischemia', 0.32721503343756336), ('necrosis', 0.2952485093230571), ('myocardial', 0.2952485093230571), ('profiling', 0.28780677826131335), ('artery', 0.2728518322024699), ('cardiac', 0.24825101755155302), ('heart', 0.20554706060265837), ('stable', 0.19585147378324902), ('diseases', 0.18360060797543937), ('diagnosis', 0.1830076907247054), ('gene', 0.16613880542640835), ('expression', 0.16566779728626801), ('disease', 0.13837642954941373), ('using', 0.09936807573509794)]\n"
     ]
    }
   ],
   "source": [
    "textTest=\"\"\"\n",
    "'Diagnosis of Coronary Heart Diseases Using Gene Expression Profiling; Stable Coronary Artery Disease, Cardiac Ischemia with and without Myocardial Necrosis.\n",
    "\"\"\"\n",
    "manualVector={}\n",
    "\n",
    "\n",
    "sklearnVector=tfidf_doc(tfidf=tfidf,text=textTest)\n",
    "print(sklearnVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF on our data\n",
    "\n",
    "We can now identify the key words in each person's publications, by creating a single document of all texts and comapring to the background frequencies. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "#get all publications for a specific ORCID\n",
    "orcidID='0000-0001-7328-4233'\n",
    "oText=''\n",
    "for p in orcidToPubmedID[orcidID]:\n",
    "    if p in pubmedText:\n",
    "        oText+=(pubmedText[p])\n",
    "res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "for r in res[0:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily do this for all ORCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=open(config.tfidfFile,'w')\n",
    "counter=0\n",
    "\n",
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    #don't really want to do this for all, so just orcid with < 100 publications!\n",
    "    if len(orcidToPubmedID[orcid])<100:\n",
    "        counter+=1\n",
    "        if counter<=5:\n",
    "            print(counter,orcid)\n",
    "            oText=''\n",
    "            for p in orcidToPubmedID[orcid]:\n",
    "                if p in pubmedText:\n",
    "                    oText+=(pubmedText[p])\n",
    "            print(len(oText))\n",
    "            %time res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "            for r in res[0:100]:\n",
    "                o.write(orcid+'\\t'+r[0]+'\\t'+str(r[1])+'\\n')\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
