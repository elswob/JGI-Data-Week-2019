{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "TFIDF (Term Frequency - Inverse Document Frequency) is a statistical method used to quantify the importance of words within a given text, compared to a background corpus.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "https://triton.ml/blog/tf-idf-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11160 publications\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import csv\n",
    "\n",
    "file = open(config.demoPubmed)\n",
    "reader = csv.reader(file,delimiter='\\t')\n",
    "pubData=[]\n",
    "for row in reader:\n",
    "    text=row[2]+' '+row[3]\n",
    "    #print(text.split())\n",
    "    pubData.append(text.lower().split())\n",
    "    \n",
    "#Removes header\n",
    "pubData = pubData[1:]\n",
    "print(len(pubData),'publications')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing a TF Map\n",
    "\n",
    "**TF(term) = # of times the term appears in document / total # of terms in document** \n",
    "\n",
    "Now that our data is usable, we’d like to start computing the TF and the IDF. Recall that computing the tf of a word in a document requires us to calculate the number of words in a review, and the number of times each word appears in the review. We can store each (word, word count pair) in a dictionary. The keys of the dictionary are then just the unique terms in the review. The following function takes in a review and outputs a tf dictionary for that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReviewTFDict(review):\n",
    "    \"\"\" Returns a tf dictionary for each review whose keys are all \n",
    "    the unique words in the review and whose values are their \n",
    "    corresponding tf.\n",
    "    \"\"\"\n",
    "    #Counts the number of times the word appears in review\n",
    "    reviewTFDict = {}\n",
    "    for word in review:\n",
    "        if word in reviewTFDict:\n",
    "            reviewTFDict[word] += 1\n",
    "        else:\n",
    "            reviewTFDict[word] = 1\n",
    "    #Computes tf for each word           \n",
    "    for word in reviewTFDict:\n",
    "        reviewTFDict[word] = reviewTFDict[word] / len(review)\n",
    "    return reviewTFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for each list\n",
    "tfDict={}\n",
    "for d in range(0,len(pubData)-1):\n",
    "    tfDict[d]=computeReviewTFDict(pubData[d])\n",
    "print(tfDict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing an IDF Map\n",
    "\n",
    "**IDF(term) = log(total # of documents / # of documents with term in it)** \n",
    "\n",
    "Computing the idf of a word requires us to compute the total number of documents and the number of documents that contains the word. In our case we can calculate the total number of documents with len(data), the number of publications. For each publication, we increment the document count for each unique word. We can use the keys of the dictionaries that we calculated in the TF step to get the unique set of words. The resulting IDF dictionary’s keys will be the set of all unique words across every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCountDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in\n",
    "    the dataset and whose values count the number of reviews in which\n",
    "    the word appears.\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    # Run through each review's tf dictionary and increment countDict's (word, doc) pair\n",
    "    for review in tfDict:\n",
    "        for word in tfDict[review]:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "\n",
    "#Stores the review count dictionary\n",
    "countDict = computeCountDict()\n",
    "countDict[\"genetic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute an idfDict, using countDict and some math, and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def computeIDFDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "        idfDict[word] = math.log(len(pubData) / countDict[word])\n",
    "    return idfDict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "idfDict = computeIDFDict()\n",
    "\n",
    "print(idfDict[\"genetic\"])\n",
    "print(idfDict[\"mendelian\"])\n",
    "print(idfDict[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF using sklearn\n",
    "\n",
    "The above has been implemented in the python package scikit-learn (sklearn) - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "And can be achieved in just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus\n",
      "orcid_to_pubmed\n",
      "load_orcid\n",
      "load_pubmed\n",
      "CPU times: user 12.8 s, sys: 516 ms, total: 13.3 s\n",
      "Wall time: 12.9 s\n",
      "Stored 'matrix' (ndarray)\n",
      "Stored 'token_dict' (dict)\n",
      "Stored 'tfs' (csr_matrix)\n",
      "Stored 'tfidf' (TfidfVectorizer)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#first lets create some functions to help process the data\n",
    "\n",
    "#load orcid to pmid data\n",
    "def load_orcid():\n",
    "    print('load_orcid')\n",
    "    orcidToPubmedID={}\n",
    "    with open(config.orcidFile) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            orcid,pmid = line.rstrip().split('\\t')\n",
    "            if orcid in orcidToPubmedID:\n",
    "                orcidToPubmedID[orcid].append(pmid)\n",
    "            else:\n",
    "                orcidToPubmedID[orcid]=[pmid]\n",
    "    return orcidToPubmedID\n",
    "\n",
    "#load the publication data\n",
    "def load_pubmed():\n",
    "    print('load_pubmed')\n",
    "    pubmedText={}\n",
    "    with open(config.pubmedFile, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            text=row[2]+' '+row[3]\n",
    "            pubmedText[row[0]]=text\n",
    "    return pubmedText\n",
    "\n",
    "#create dictionary of orcid to publication text\n",
    "def orcid_to_pubmed():\n",
    "    print('orcid_to_pubmed')\n",
    "    orcidToPubmedID=load_orcid()\n",
    "    pubmedText = load_pubmed()\n",
    "    orcidToPubmed={}\n",
    "    for orcid in orcidToPubmedID:\n",
    "        oText=''\n",
    "        for p in orcidToPubmedID[orcid]:\n",
    "            if p in pubmedText:\n",
    "                oText+=(pubmedText[p])\n",
    "        orcidToPubmed[orcid]=oText\n",
    "    return orcidToPubmed\n",
    "\n",
    "print('Reading corpus')\n",
    "token_dict = {}\n",
    "orcidToPubmed = orcid_to_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    token_dict[orcid] = orcidToPubmed[orcid].lower()\n",
    "\n",
    "#sklean tokeniser, including bigrams and trigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
    "\n",
    "#fit_transform creates the tf-idf model and returns term-document frequency matrix\n",
    "%time tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "#get similarity matrix for all people\n",
    "#https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "matrix=(tfs * tfs.T).A\n",
    "#store this and the dictionary for access in other notebooks\n",
    "%store matrix\n",
    "%store token_dict\n",
    "%store tfs\n",
    "%store tfidf\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to test some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mendelian', 0.8418234170672653)\n",
      "('genetic', 0.5397530310032479)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_doc(tfidf='',text=''):\n",
    "    text=text.lower()\n",
    "    #transform function transforms a document to document-term matrix\n",
    "    response = tfidf.transform([text])\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    res={}\n",
    "    for col in response.nonzero()[1]:\n",
    "        res[feature_names[col]]=response[0, col]\n",
    "        #reverse sort the results\n",
    "        sorted_res = sorted(res.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return sorted_res\n",
    "\n",
    "sorted_res=tfidf_doc(tfidf=tfidf,text='genetic mendelian the')\n",
    "for s in sorted_res:\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF on our data\n",
    "\n",
    "We can now identify the key words in each person's publications, by creating a single document of all texts and comapring to the background frequencies. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_orcid\n",
      "load_pubmed\n",
      "('nematode', 0.14027902569742493)\n",
      "('mammary', 0.14027902569742493)\n",
      "('ccr5', 0.13777851866416813)\n",
      "('motu', 0.11492279324093777)\n",
      "('crpc', 0.10716107007213078)\n",
      "('cell', 0.09402472111432539)\n",
      "('cancer', 0.09346441177633637)\n",
      "('mammary stem', 0.0918523457761121)\n",
      "('id4', 0.0918523457761121)\n",
      "('genome', 0.0856179178746168)\n"
     ]
    }
   ],
   "source": [
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "#get all publications for a specific ORCID\n",
    "orcidID='0000-0001-7328-4233'\n",
    "oText=''\n",
    "for p in orcidToPubmedID[orcidID]:\n",
    "    if p in pubmedText:\n",
    "        oText+=(pubmedText[p])\n",
    "res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "for r in res[0:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily do this for all ORCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_orcid\n",
      "load_pubmed\n",
      "1 0000-0001-5001-3350\n",
      "5877\n",
      "CPU times: user 2.45 s, sys: 63.9 ms, total: 2.52 s\n",
      "Wall time: 2.53 s\n",
      "2 0000-0001-8347-5092\n",
      "26148\n",
      "CPU times: user 8.85 s, sys: 84 ms, total: 8.94 s\n",
      "Wall time: 8.97 s\n",
      "3 0000-0002-8570-0406\n",
      "4913\n",
      "CPU times: user 2.68 s, sys: 64.5 ms, total: 2.74 s\n",
      "Wall time: 2.75 s\n",
      "4 0000-0002-3091-3164\n",
      "1228\n",
      "CPU times: user 2.42 s, sys: 43.9 ms, total: 2.46 s\n",
      "Wall time: 2.47 s\n",
      "5 0000-0001-6563-9903\n",
      "2452\n",
      "CPU times: user 2.45 s, sys: 45.8 ms, total: 2.5 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "o=open('output/orcid-tf-idf.txt','w')\n",
    "counter=0\n",
    "\n",
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    #don't really want to do this for all, so just orcid with < 100 publications!\n",
    "    if len(orcidToPubmedID[orcid])<100:\n",
    "        counter+=1\n",
    "        if counter<=5:\n",
    "            print(counter,orcid)\n",
    "            oText=''\n",
    "            for p in orcidToPubmedID[orcid]:\n",
    "                if p in pubmedText:\n",
    "                    oText+=(pubmedText[p])\n",
    "            print(len(oText))\n",
    "            %time res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "            for r in res[0:100]:\n",
    "                o.write(orcid+'\\t'+r[0]+'\\t'+str(r[1])+'\\n')\n",
    "o.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
