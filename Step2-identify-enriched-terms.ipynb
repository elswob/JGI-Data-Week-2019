{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "TFIDF (Term Frequency - Inverse Document Frequency) is a statistical method used to quantify the importance of words within a given text, compared to a background corpus.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "https://triton.ml/blog/tf-idf-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF from scratch\n",
    "\n",
    "First, lets read in the complete set of publication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import csv\n",
    "\n",
    "file = open(config.demoPubmedFile)\n",
    "reader = csv.reader(file,delimiter='\\t')\n",
    "pubData=[]\n",
    "for row in reader:\n",
    "    text=row[2]+' '+row[3]\n",
    "    pubData.append(text.lower().split())\n",
    "    \n",
    "#Removes header\n",
    "pubData = pubData[1:]\n",
    "print(len(pubData),'publications')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing a TF Map\n",
    "\n",
    "**TF(term) = # of times the term appears in document / total # of terms in document** \n",
    "\n",
    "Now that our data is usable, we’d like to start computing the TF and the IDF. Computing the tf of a word in a publication requires us to calculate the number of words in a publication, and the number of times each word appears in the publication. We can store each (word, word count pair) in a dictionary. The keys of the dictionary are then just the unique terms in the publication. The following function takes in a publication and outputs a tf dictionary for that publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePublicationTFDict(publication):\n",
    "    \"\"\" Returns a tf dictionary for each publication whose keys are all \n",
    "    the unique words in the publication and whose values are their \n",
    "    corresponding tf.\n",
    "    \"\"\"\n",
    "    #Counts the number of times the word appears in publication\n",
    "    publicationTFDict = {}\n",
    "    for word in publication:\n",
    "        if word in publicationTFDict:\n",
    "            publicationTFDict[word] += 1\n",
    "        else:\n",
    "            publicationTFDict[word] = 1\n",
    "    #Computes tf for each word           \n",
    "    for word in publicationTFDict:\n",
    "        publicationTFDict[word] = publicationTFDict[word] / len(publication)\n",
    "    return publicationTFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sixty-five': 0.0041841004184100415, 'common': 0.0041841004184100415, 'genetic': 0.008368200836820083, 'variants': 0.0041841004184100415, 'and': 0.016736401673640166, 'prediction': 0.0041841004184100415, 'of': 0.029288702928870293, 'type': 0.008368200836820083, '2': 0.008368200836820083, 'diabetes.': 0.0041841004184100415, 'we': 0.008368200836820083, 'developed': 0.008368200836820083, 'a': 0.02092050209205021, '65': 0.0041841004184100415, 'diabetes': 0.0041841004184100415, '(t2d)': 0.0041841004184100415, 'variant-weighted': 0.0041841004184100415, 'gene': 0.016736401673640166, 'score': 0.02092050209205021, 'to': 0.029288702928870293, 'examine': 0.0041841004184100415, 'the': 0.06276150627615062, 'impact': 0.0041841004184100415, 'on': 0.0041841004184100415, 't2d': 0.016736401673640166, 'risk': 0.02092050209205021, 'assessment': 0.0041841004184100415, 'in': 0.012552301255230125, 'u.k.-based': 0.0041841004184100415, 'consortium': 0.0041841004184100415, 'prospective': 0.0041841004184100415, 'studies,': 0.0041841004184100415, 'with': 0.012552301255230125, 'subjects': 0.0041841004184100415, 'initially': 0.0041841004184100415, 'free': 0.0041841004184100415, 'from': 0.0041841004184100415, '(n': 0.0041841004184100415, '=': 0.02092050209205021, '13,294;': 0.0041841004184100415, '37.3%': 0.0041841004184100415, 'women;': 0.0041841004184100415, 'mean': 0.0041841004184100415, 'age': 0.008368200836820083, '58.5': 0.0041841004184100415, '[38-99]': 0.0041841004184100415, 'years).': 0.0041841004184100415, 'compared': 0.0041841004184100415, 'performance': 0.0041841004184100415, 'phenotypically': 0.0041841004184100415, 'derived': 0.0041841004184100415, 'framingham': 0.008368200836820083, 'offspring': 0.0041841004184100415, 'study': 0.0041841004184100415, 'model': 0.012552301255230125, 'then': 0.0041841004184100415, 'two': 0.0041841004184100415, 'combination.': 0.0041841004184100415, 'over': 0.0041841004184100415, 'median': 0.0041841004184100415, '10': 0.0041841004184100415, 'years': 0.0041841004184100415, 'follow-up,': 0.0041841004184100415, '804': 0.0041841004184100415, 'participants': 0.0041841004184100415, 't2d.': 0.008368200836820083, 'odds': 0.0041841004184100415, 'ratio': 0.0041841004184100415, 'for': 0.0041841004184100415, '(top': 0.0041841004184100415, 'vs.': 0.0041841004184100415, 'bottom': 0.0041841004184100415, 'quintiles': 0.0041841004184100415, 'score)': 0.0041841004184100415, 'was': 0.008368200836820083, '2.70': 0.0041841004184100415, '(95%': 0.016736401673640166, 'ci': 0.029288702928870293, '2.12-3.43).': 0.0041841004184100415, '10%': 0.0041841004184100415, 'false-positive': 0.0041841004184100415, 'rate,': 0.0041841004184100415, 'alone': 0.0041841004184100415, 'detected': 0.0041841004184100415, '19.9%': 0.0041841004184100415, 'incident': 0.008368200836820083, 'cases,': 0.0041841004184100415, '30.7%,': 0.0041841004184100415, 'together': 0.0041841004184100415, '37.3%.': 0.0041841004184100415, 'respective': 0.0041841004184100415, 'area': 0.0041841004184100415, 'under': 0.0041841004184100415, 'receiver': 0.0041841004184100415, 'operator': 0.0041841004184100415, 'characteristic': 0.0041841004184100415, 'curves': 0.0041841004184100415, 'were': 0.0041841004184100415, '0.60': 0.0041841004184100415, '0.58-0.62),': 0.0041841004184100415, '0.75': 0.008368200836820083, '0.73': 0.0041841004184100415, '0.77),': 0.0041841004184100415, '0.76': 0.0041841004184100415, '0.78).': 0.0041841004184100415, 'combined': 0.0041841004184100415, 'net': 0.0041841004184100415, 'reclassification': 0.0041841004184100415, 'improvement': 0.008368200836820083, '(nri)': 0.0041841004184100415, '8.1%': 0.0041841004184100415, '(5.0': 0.0041841004184100415, '11.2;': 0.0041841004184100415, 'p': 0.016736401673640166, '3.31': 0.0041841004184100415, '×': 0.012552301255230125, '10(-7)).': 0.0041841004184100415, 'while': 0.0041841004184100415, 'bmi': 0.0041841004184100415, 'stratification': 0.0041841004184100415, 'into': 0.0041841004184100415, 'tertiles': 0.0041841004184100415, 'influenced': 0.0041841004184100415, 'nri': 0.0041841004184100415, '(bmi': 0.0041841004184100415, '≤24.5': 0.0041841004184100415, 'kg/m(2),': 0.012552301255230125, '27.6%': 0.0041841004184100415, '[95%': 0.012552301255230125, '17.7-37.5],': 0.0041841004184100415, '4.82': 0.0041841004184100415, '10(-8);': 0.0041841004184100415, '24.5-27.5': 0.0041841004184100415, '11.6%': 0.0041841004184100415, '5.8-17.4],': 0.0041841004184100415, '9.88': 0.0041841004184100415, '10(-5);': 0.0041841004184100415, '>27.5': 0.0041841004184100415, '2.6%': 0.0041841004184100415, '-1.4': 0.0041841004184100415, '6.6],': 0.0041841004184100415, '0.20),': 0.0041841004184100415, 'categories': 0.0041841004184100415, 'did': 0.0041841004184100415, 'not.': 0.0041841004184100415, 'addition': 0.0041841004184100415, 'phenotypic': 0.0041841004184100415, 'leads': 0.0041841004184100415, 'potentially': 0.0041841004184100415, 'clinically': 0.0041841004184100415, 'important': 0.0041841004184100415, 'discrimination': 0.0041841004184100415}\n"
     ]
    }
   ],
   "source": [
    "#run for each list\n",
    "tfDict={}\n",
    "for d in range(0,len(pubData)):\n",
    "    tfDict[d]=computePublicationTFDict(pubData[d])\n",
    "print(tfDict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing an IDF Map\n",
    "\n",
    "**IDF(term) = log(total # of documents / # of documents with term in it)** \n",
    "\n",
    "Computing the idf of a word requires us to compute the total number of documents and the number of documents that contains the word. In our case we can calculate the total number of documents with len(data), the number of publications. For each publication, we increment the document count for each unique word. We can use the keys of the dictionaries that we calculated in the TF step to get the unique set of words. The resulting IDF dictionary’s keys will be the set of all unique words across every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetic 1001\n"
     ]
    }
   ],
   "source": [
    "def computeCountDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in\n",
    "    the dataset and whose values count the number of reviews in which\n",
    "    the word appears.\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    # Run through each publications's tf dictionary and increment countDict's (word, doc) pair\n",
    "    for review in tfDict:\n",
    "        for word in tfDict[review]:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "\n",
    "#Stores the publication count dictionary\n",
    "countDict = computeCountDict()\n",
    "testWord='genetic'\n",
    "print(testWord,countDict[testWord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute an idfDict, using countDict and some math, and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4113364566200812\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDFDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "        idfDict[word] = math.log(len(pubData) / countDict[word])\n",
    "    return idfDict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "idfDict = computeIDFDict()\n",
    "\n",
    "print(idfDict[\"genetic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there are 11,160 publictions, and the word genetic is mentioned 1,001 times. Therefore the idf = log(11160/1001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the TF-IDF Map\n",
    "\n",
    "**TF-IDF(term) = TF(term) * IDF(term)**\n",
    "\n",
    "The last step is to compute the TF-IDF. We use our existing tf dictionaries and simply multiply each value by the idf. We can use the idf keys since they contain every unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sixty-five': 0.03609600023169605, 'common': 0.01105860993593375, 'genetic': 0.020178547754142937, 'variants': 0.013644217742064873, 'and': 0.0010234509476662228, 'prediction': 0.01862994470912016, 'of': 0.0009847888075354757, 'type': 0.025659099785437893, '2': 0.024545706414833945, 'diabetes.': 0.020215292327209883, 'we': 0.006099618113897391, 'developed': 0.02719124274420763, 'a': 0.004423669849068166, '65': 0.02412026432822547, 'diabetes': 0.01613497669836691, '(t2d)': 0.029361950388875548, 'variant-weighted': 0.03899619763989665, 'gene': 0.04483529817899097, 'score': 0.09205251694784888, 'to': 0.0039019646067503305, 'examine': 0.012372649089518146, 'the': 0.003748328683648812, 'impact': 0.01162787128345355, 'on': 0.003454195544283977, 't2d': 0.11181646705138147, 'risk': 0.04028447968025657, 'assessment': 0.014681800179743102, 'in': 0.0013262666868937099, 'u.k.-based': 0.03609600023169605, 'consortium': 0.021935371514768126, 'prospective': 0.01349065553766978, 'studies,': 0.016486166871944947, 'with': 0.005216697577890016, 'subjects': 0.019291050354070995, 'initially': 0.02159501319069301, 'free': 0.018144287088815755, 'from': 0.0037853946052960217, '(n': 0.016524908819278823, '=': 0.05906519342572049, '13,294;': 0.03899619763989665, '37.3%': 0.034399493503210005, 'women;': 0.029802789366523356, 'mean': 0.015089365817681727, 'age': 0.023538188481592386, '58.5': 0.03609600023169605, '[38-99]': 0.03899619763989665, 'years).': 0.02424155109338552, 'compared': 0.010043545548484899, 'performance': 0.015556369937471132, 'phenotypically': 0.03226214779707615, 'derived': 0.015215529157308033, 'framingham': 0.0645242955941523, 'offspring': 0.01644778036032971, 'study': 0.005466180305868691, 'model': 0.032042863313848244, 'then': 0.014426667139999523, 'two': 0.008697593943987381, 'combination.': 0.029802789366523356, 'over': 0.010676142361032101, 'median': 0.020168542115502244, '10': 0.015810994522078976, 'years': 0.012329587641666482, 'follow-up,': 0.026902591958322748, '804': 0.03899619763989665, 'participants': 0.014860963347872508, 't2d.': 0.06879898700642001, 'odds': 0.018503144083616696, 'ratio': 0.01544966578345048, 'for': 0.001887510655484351, '(top': 0.03319580282349544, 'vs.': 0.020122308491290593, 'bottom': 0.026062965617476928, 'quintiles': 0.034399493503210005, 'score)': 0.030854314171045973, 'was': 0.010042656135650177, '2.70': 0.03899619763989665, '(95%': 0.06898065614509583, 'ci': 0.1282390865827698, '2.12-3.43).': 0.03899619763989665, '10%': 0.02254504436489948, 'false-positive': 0.029802789366523356, 'rate,': 0.022627900546055044, 'alone': 0.019727703137854435, 'detected': 0.016623362783338213, '19.9%': 0.03899619763989665, 'incident': 0.040717522507797435, 'cases,': 0.020609381093150053, '30.7%,': 0.03899619763989665, 'together': 0.016623362783338213, '37.3%.': 0.03899619763989665, 'respective': 0.023667487823454625, 'area': 0.016486166871944947, 'under': 0.01309529636017638, 'receiver': 0.0276654436603895, 'operator': 0.029361950388875548, 'characteristic': 0.019942319432363017, 'curves': 0.02625761003435932, 'were': 0.005749713963691084, '0.60': 0.03226214779707615, '0.58-0.62),': 0.03899619763989665, '0.75': 0.0629985921900188, '0.73': 0.0314992960950094, '0.77),': 0.03609600023169605, '0.76': 0.0314992960950094, '0.78).': 0.03899619763989665, 'combined': 0.014993803089774518, 'net': 0.02424155109338552, 'reclassification': 0.034399493503210005, 'improvement': 0.04232143396564005, '(nri)': 0.03899619763989665, '8.1%': 0.03899619763989665, '(5.0': 0.03609600023169605, '11.2;': 0.03899619763989665, 'p': 0.06041279026879384, '3.31': 0.03899619763989665, '×': 0.055509432250850095, '10(-7)).': 0.03226214779707615, 'while': 0.012100359678873246, 'bmi': 0.018319869511824256, 'stratification': 0.024907093748739867, 'into': 0.009169854770942534, 'tertiles': 0.03899619763989665, 'influenced': 0.017735091510254555, 'nri': 0.03899619763989665, '(bmi': 0.029802789366523356, '≤24.5': 0.03899619763989665, 'kg/m(2),': 0.10828800069508815, '27.6%': 0.03609600023169605, '[95%': 0.07068466671742299, '17.7-37.5],': 0.03899619763989665, '4.82': 0.03899619763989665, '10(-8);': 0.03609600023169605, '24.5-27.5': 0.03899619763989665, '11.6%': 0.03609600023169605, '5.8-17.4],': 0.03899619763989665, '9.88': 0.03899619763989665, '10(-5);': 0.03899619763989665, '>27.5': 0.03899619763989665, '2.6%': 0.03226214779707615, '-1.4': 0.034399493503210005, '6.6],': 0.03899619763989665, '0.20),': 0.03899619763989665, 'categories': 0.022798703870407576, 'did': 0.013324378011939593, 'not.': 0.022886793448641186, 'addition': 0.016390852945870386, 'phenotypic': 0.019523560191538822, 'leads': 0.01689074270793855, 'potentially': 0.015389891903753925, 'clinically': 0.018503144083616696, 'important': 0.00937617925240851, 'discrimination': 0.022384097583193218}\n"
     ]
    }
   ],
   "source": [
    "def computeReviewTFIDFDict(reviewTFDict):\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    review and whose values are their corresponding tfidf.\n",
    "    \"\"\"\n",
    "    reviewTFIDFDict = {}\n",
    "    #For each word in the publication, we multiply its tf and its idf.\n",
    "    for word in tfDict[reviewTFDict]:\n",
    "        reviewTFIDFDict[word] = tfDict[reviewTFDict][word] * idfDict[word]\n",
    "    return reviewTFIDFDict\n",
    "\n",
    "#Stores the TF-IDF dictionaries\n",
    "tfidfDict = [computeReviewTFIDFDict(review) for review in tfDict]\n",
    "print(tfidfDict[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using the **genetic** example before, for publication 1, we had a tf value of **'genetic': 0.008368200836820083** and the idf is 2.4113364566200812. Multiply these together, and you get 0.02 (ish) as seen above :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF using sklearn\n",
    "\n",
    "The above has been implemented in the python package scikit-learn (sklearn) - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "And can be achieved in just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus\n",
      "orcid_to_pubmed\n",
      "load_orcid\n",
      "load_pubmed\n",
      "CPU times: user 14.2 s, sys: 660 ms, total: 14.9 s\n",
      "Wall time: 13.2 s\n",
      "Stored 'matrix' (ndarray)\n",
      "Stored 'token_dict' (dict)\n",
      "Stored 'tfs' (csr_matrix)\n",
      "Stored 'tfidf' (TfidfVectorizer)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#first lets create some functions to help process the data\n",
    "\n",
    "#load orcid to pmid data\n",
    "def load_orcid():\n",
    "    print('load_orcid')\n",
    "    orcidToPubmedID={}\n",
    "    with open(config.demoOrcidFile) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            orcid,pmid = line.rstrip().split('\\t')\n",
    "            if orcid in orcidToPubmedID:\n",
    "                orcidToPubmedID[orcid].append(pmid)\n",
    "            else:\n",
    "                orcidToPubmedID[orcid]=[pmid]\n",
    "    return orcidToPubmedID\n",
    "\n",
    "#load the publication data\n",
    "def load_pubmed():\n",
    "    print('load_pubmed')\n",
    "    pubmedText={}\n",
    "    with open(config.demoPubmedFile, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            text=row[2]+' '+row[3]\n",
    "            pubmedText[row[0]]=text\n",
    "    return pubmedText\n",
    "\n",
    "#create dictionary of orcid to publication text\n",
    "def orcid_to_pubmed():\n",
    "    print('orcid_to_pubmed')\n",
    "    orcidToPubmedID=load_orcid()\n",
    "    pubmedText = load_pubmed()\n",
    "    orcidToPubmed={}\n",
    "    for orcid in orcidToPubmedID:\n",
    "        oText=''\n",
    "        for p in orcidToPubmedID[orcid]:\n",
    "            if p in pubmedText:\n",
    "                oText+=(pubmedText[p])\n",
    "        orcidToPubmed[orcid]=oText\n",
    "    return orcidToPubmed\n",
    "\n",
    "print('Reading corpus')\n",
    "token_dict = {}\n",
    "orcidToPubmed = orcid_to_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    token_dict[orcid] = orcidToPubmed[orcid].lower()\n",
    "\n",
    "#sklean including bigrams and trigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
    "\n",
    "#fit_transform creates the tf-idf model and returns term-document frequency matrix\n",
    "%time tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "#get similarity matrix for all people\n",
    "#https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "matrix=(tfs * tfs.T).A\n",
    "#store this and the dictionary for access in other notebooks\n",
    "%store matrix\n",
    "%store token_dict\n",
    "%store tfs\n",
    "%store tfidf\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to test a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t2d', 0.2771374468347099)\n",
      "('gene score', 0.18732077434507402)\n",
      "('95 ci', 0.1680563752219566)\n",
      "('ci', 0.16519742224518033)\n",
      "('95', 0.14483197609614537)\n",
      "('score', 0.14159779049586887)\n",
      "('risk model', 0.12883139598887866)\n",
      "('nri', 0.09366038717253701)\n",
      "('framingham', 0.09366038717253701)\n",
      "('27 kg', 0.09064520376269428)\n",
      "('genetic', 0.03563686144053027)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_doc(tfidf='',text=''):\n",
    "    text=text.lower()\n",
    "    #transform function transforms a document to document-term matrix\n",
    "    response = tfidf.transform([text])\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    res={}\n",
    "    for col in response.nonzero()[1]:\n",
    "        res[feature_names[col]]=response[0, col]\n",
    "        #reverse sort the results\n",
    "        sorted_res = sorted(res.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return sorted_res\n",
    "\n",
    "#sorted_res=tfidf_doc(tfidf=tfidf,text='genetic mendelian the')\n",
    "testText=\",\".join(pubData[0])\n",
    "#print(testText)\n",
    "sorted_res=tfidf_doc(tfidf=tfidf,text=testText)\n",
    "for s in sorted_res[:10]:\n",
    "    print(s)\n",
    "\n",
    "#get the genetic score\n",
    "for s in sorted_res:\n",
    "    #print(s[0])\n",
    "    if s[0] == 'genetic':\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why the different tf-idf scores to the first time?\n",
    "\n",
    "The first example treated each publication separately. In the sklearn example above, each person's collection of publications is treated as a single record. This does mean that there are duplicate publications in the model. Combine this with the slight variation of tf-idf in sklearn, the use of bigrams and trigrams, and the stopwords removal in sklearn, and this might explain the difference. \n",
    "\n",
    "For example, the top term in sklean is **t2d** (type 2 diabetes). However, in the manual tf-idf, method this is split across three results **(t2d)**, **td.** and **td**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF on our data\n",
    "\n",
    "We can now identify the key words in each person's publications, by creating a single document of all texts and comapring to the background frequencies. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_orcid\n",
      "load_pubmed\n",
      "('nematode', 0.1399503126881953)\n",
      "('mammary', 0.1399503126881953)\n",
      "('ccr5', 0.13745566504259402)\n",
      "('motu', 0.11465349697937983)\n",
      "('crpc', 0.10690996169979536)\n",
      "('cell', 0.09380439488333107)\n",
      "('cancer', 0.09324539850690329)\n",
      "('mammary stem', 0.09163711002839603)\n",
      "('id4', 0.09163711002839603)\n",
      "('genome', 0.08541729113595346)\n"
     ]
    }
   ],
   "source": [
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "#get all publications for a specific ORCID\n",
    "orcidID='0000-0001-7328-4233'\n",
    "oText=''\n",
    "for p in orcidToPubmedID[orcidID]:\n",
    "    if p in pubmedText:\n",
    "        oText+=(pubmedText[p])\n",
    "res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "for r in res[0:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily do this for all ORCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_orcid\n",
      "load_pubmed\n",
      "1 0000-0001-5001-3350\n",
      "1959\n",
      "CPU times: user 2.58 s, sys: 37.4 ms, total: 2.62 s\n",
      "Wall time: 2.63 s\n",
      "2 0000-0001-5008-0705\n",
      "378\n",
      "CPU times: user 2.45 s, sys: 39.1 ms, total: 2.49 s\n",
      "Wall time: 2.49 s\n",
      "3 0000-0001-5017-9473\n",
      "9903\n",
      "CPU times: user 3.44 s, sys: 44.8 ms, total: 3.48 s\n",
      "Wall time: 3.48 s\n",
      "4 0000-0001-5031-7493\n",
      "3435\n",
      "CPU times: user 2.6 s, sys: 38.2 ms, total: 2.63 s\n",
      "Wall time: 2.64 s\n",
      "5 0000-0001-5052-3182\n",
      "1935\n",
      "CPU times: user 2.6 s, sys: 45.7 ms, total: 2.65 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "o=open(config.tfidfFile,'w')\n",
    "counter=0\n",
    "\n",
    "orcidToPubmedID=load_orcid()\n",
    "pubmedText = load_pubmed()\n",
    "for orcid in orcidToPubmed:\n",
    "    #don't really want to do this for all, so just orcid with < 100 publications!\n",
    "    if len(orcidToPubmedID[orcid])<100:\n",
    "        counter+=1\n",
    "        if counter<=5:\n",
    "            print(counter,orcid)\n",
    "            oText=''\n",
    "            for p in orcidToPubmedID[orcid]:\n",
    "                if p in pubmedText:\n",
    "                    oText+=(pubmedText[p])\n",
    "            print(len(oText))\n",
    "            %time res = tfidf_doc(tfidf=tfidf,text=oText)\n",
    "            for r in res[0:100]:\n",
    "                o.write(orcid+'\\t'+r[0]+'\\t'+str(r[1])+'\\n')\n",
    "o.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
