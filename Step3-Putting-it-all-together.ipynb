{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To recap\n",
    "\n",
    "### Data so far\n",
    "\n",
    "1. We have created a list of people and their ORCID identifiers - [output/pure_person_to_orcid.txt](output/pure_person_to_orcid.txt)\n",
    "2. We have created a list of ORCID to PubMed identifiers - [output/orcid.tsv](output/orcid.tsv)\n",
    "3. We have created a list of PubMed IDs to PubMed info - [output/pubmed.tsv](output/pubmed.tsv)\n",
    "4. We have created a list of the top 100 terms for each person - [output/orcid-tf-idf.txt](output/orcid-tf-idf.txt)\n",
    "\n",
    "### Questions\n",
    "\n",
    "Now we need to use the common identifiers in these data to answer some questions:\n",
    "\n",
    "1. Can we produce a set of potential collaborators for each person, a collaborator being someone they have significant terms in common with but not previously published with.\n",
    "2. Can we select a set of people that most closely map to a specific piece of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas dataframes\n",
    "\n",
    "Normally at this point we would use a database. However, to keep it all in a single language and framework, and as the data themselves are relatively small, we can use pandas - https://pandas.pydata.org/\n",
    "\n",
    ">pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, following on from the data descriptions above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. People and their ORCID identifiers \n",
    "personToOrcid = pd.read_csv('data/pure_person_to_orcid.txt',sep='\\t')\n",
    "print(personToOrcid.shape)\n",
    "print(personToOrcid.head())\n",
    "\n",
    "# 2. ORCID to PubMed identifiers\n",
    "orcidToPubmed = pd.read_csv('data/orcid.tsv',sep='\\t')\n",
    "print(orcidToPubmed.shape)\n",
    "print(orcidToPubmed.head())\n",
    "\n",
    "# 3. PubMed IDs to PubMed info\n",
    "pubmedToInfo = pd.read_csv('data/pubmed.tsv',sep='\\t')\n",
    "print(pubmedToInfo.shape)\n",
    "print(pubmedToInfo.head())\n",
    "\n",
    "# 4. Top 100 terms for each person\n",
    "topTerms = pd.read_csv('data/orcid-tf-idf.txt',sep='\\t')\n",
    "print(topTerms.shape)\n",
    "print(topTerms.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first pass would be to count the number of matching terms between each person. To start with, let's just compare the first in the list to everyone else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #loop through people (limit to top 1)\n",
    "def compare_people(limit):\n",
    "    for i, iData in personToOrcid.head(n=limit).iterrows():\n",
    "        print('### Comparing',i,iData.pure_person_id,iData.orcid_id)\n",
    "        #get all terms and tf-idf values\n",
    "        iTopTerms=topTerms[topTerms['orcid_id']==iData.orcid_id]\n",
    "        #print(iTopTerms.head())\n",
    "        iTerms = iTopTerms['term']\n",
    "\n",
    "        for j in range(i+1,personToOrcid.shape[0]):\n",
    "            jData=personToOrcid.iloc[j]\n",
    "            jTopTerms=topTerms[topTerms['orcid_id']==jData.orcid_id]\n",
    "            #print(jTopTerms['term'].head())\n",
    "            jTerms = jTopTerms['term']\n",
    "            #print(iTerms,jTerms)\n",
    "            com = list(set(iTerms) & set(jTerms))\n",
    "            #only show maches with more than 1 matching term\n",
    "            if len(com) > 1:\n",
    "                print(jData.orcid_id,com)\n",
    "    \n",
    "compare_people(1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few matches with many overlapping terms, except one **0000-0001-7086-8882**. Let's look at the research pages of these two people\n",
    "\n",
    "https://research-information.bristol.ac.uk/en/persons/melody-a-s-sylvestre(81e7f06c-77d8-4020-9608-0b30dd001c43)/publications.html https://research-information.bristol.ac.uk/en/persons/nicholas-a-teanby(4ec18a96-fd2e-4311-a6fa-7ec65696a4e9)/publications.html \n",
    "\n",
    "This suggests they are from similar research areas, and have indeed co-published.   \n",
    "\n",
    "Let's try again, this time with the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_people(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second person matched no-one, the third however matched quite a few. Let's modify the function to sort by number:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_people(limit):\n",
    "    for i, iData in personToOrcid.head(n=limit).iterrows():\n",
    "        print('### Comparing',i,iData.pure_person_id,iData.orcid_id)\n",
    "        #get all terms and tf-idf values\n",
    "        iTopTerms=topTerms[topTerms['orcid_id']==iData.orcid_id]\n",
    "        iTerms = iTopTerms['term']\n",
    "        jComp={}\n",
    "        for j in range(i+1,personToOrcid.shape[0]):\n",
    "            jData=personToOrcid.iloc[j]\n",
    "            jTopTerms=topTerms[topTerms['orcid_id']==jData.orcid_id]\n",
    "            jTerms = jTopTerms['term']\n",
    "            com = list(set(iTerms) & set(jTerms))\n",
    "            #only show maches with more than 1 matching term\n",
    "            if len(com) > 1:\n",
    "                jComp[jData.orcid_id]=com\n",
    "        #create sorted dictionary using number of items\n",
    "        jComp = sorted(jComp.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "        for p in jComp[0:10]:\n",
    "            print(p)\n",
    "            \n",
    "compare_people(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only shows us terms that are in common. It could be possible to have lots of terms in common, but these terms are not the 'most' representative of either person. It would be better to compare all terms, and their tf-idf values.\n",
    "\n",
    "Back in the TF-IDF section (http://localhost:8888/lab#TF-IDF-using-sklearn) we created a similarity matrix from the TF-IDF model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity matrix for all people\n",
    "\n",
    "matrixCom = {}\n",
    "\n",
    "#get tfidf matrix and orcidText dictionary\n",
    "%store -r matrix\n",
    "%store -r token_dict\n",
    "\n",
    "for i in range(0,len(matrix)):\n",
    "    iOrcid=list(token_dict)[i]\n",
    "    matrixCom[iOrcid]={}\n",
    "    for j in range(0,len(matrix)):\n",
    "        jOrcid=list(token_dict)[j]\n",
    "        matrixCom[iOrcid][jOrcid]=matrix[i][j]\n",
    "\n",
    "o = open('output/orcid-to-orcid-tf-idf.tsv','w')\n",
    "o.write('orcid_1\\torcid_2\\ttf-idf\\n')\n",
    "counter=0\n",
    "for m in matrixCom:\n",
    "    sorted_res = sorted(matrixCom[m].items(), key=lambda kv: kv[1], reverse=True)\n",
    "    if counter<3:\n",
    "        print(m)\n",
    "        for s in sorted_res[0:5]:\n",
    "            print('\\t',s[0],s[1])\n",
    "        counter+=1\n",
    "    for s in sorted_res:\n",
    "        o.write(m+'\\t'+s[0]+'\\t'+str(s[1])+'\\n')\n",
    "o.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration recommendation engine\n",
    "\n",
    "This similarity data depicts the similarity between each person's publication text based on tf-idf. Often, similarities arise due to co-publication, and perhaps a more informative recommender would be to idenfity cases where people have signficant overlap in their publication text, but have never previously co-published. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the new data into a dataframe\n",
    "\n",
    "orcidToOrcid = pd.read_csv('data/orcid-to-orcid-tf-idf.tsv',sep='\\t')\n",
    "print(orcidToOrcid.shape)\n",
    "print(orcidToOrcid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching text to people\n",
    "\n",
    "Using the same tf-idf model, we can try and match any piece of text to the most relevant people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/55677314/using-sklearn-how-do-i-calculate-the-tf-idf-cosine-similarity-between-documents\n",
    "%store -r tfs\n",
    "corpus_tfidf = tfs\n",
    "%store -r tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529\n",
      "(353, 0.1498055959132002) 0000-0002-2515-0852\n",
      "(0, 0.14495921424895872) 0000-0003-0924-3247\n",
      "(321, 0.14106425254783597) 0000-0002-1753-3730\n",
      "(506, 0.09991253199428538) 0000-0003-4638-601X\n",
      "(396, 0.08854748685568083) 0000-0001-7240-4563\n",
      "(140, 0.0878923678564203) 0000-0001-6080-480X\n",
      "(16, 0.08521416949481442) 0000-0002-8193-9559\n",
      "(494, 0.07221759249818203) 0000-0002-3180-6993\n",
      "(50, 0.06675288084209205) 0000-0003-2278-5892\n",
      "(231, 0.06434504244860838) 0000-0001-6648-3007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "#26930047\n",
    "query=\"\"\"\n",
    "Diagnosis of Coronary Heart Diseases Using Gene Expression Profiling; Stable Coronary Artery Disease, Cardiac Ischemia with and without Myocardial Necrosis.    \n",
    "Cardiovascular disease (including coronary artery disease and myocardial infarction) is one of the leading causes of death in Europe, and is influenced by \n",
    "both environmental and genetic factors. With the recent advances in genomic tools and technologies there is potential to predict and diagnose heart \n",
    "disease using molecular data from analysis of blood cells. We analyzed gene expression data from blood samples taken from normal people (n = 21), \n",
    "non-significant coronary artery disease (n = 93), patients with unstable angina (n = 16), stable coronary artery disease (n = 14) and myocardial \n",
    "infarction (MI; n = 207). We used a feature selection approach to identify a set of gene expression variables which successfully differentiate different \n",
    "cardiovascular diseases. The initial features were discovered by fitting a linear model for each probe set across all arrays of normal individuals and \n",
    "patients with myocardial infarction. Three different feature optimisation algorithms were devised which identified two discriminating sets of genes, \n",
    "one using MI and normal controls (total genes = 6) and another one using MI and unstable angina patients (total genes = 7). In all our classification \n",
    "approaches we used a non-parametric k-nearest neighbour (KNN) classification method (k = 3). The results proved the diagnostic robustness of the final \n",
    "feature sets in discriminating patients with myocardial infarction from healthy controls. Interestingly it also showed efficacy in discriminating \n",
    "myocardial infarction patients from patients with clinical symptoms of cardiac ischemia but no myocardial necrosis or stable coronary artery disease, \n",
    "despite the influence of batch effects and different microarray gene chips and platforms.\"\n",
    "\"\"\"\n",
    "query_tfidf = tfidf.transform([query])\n",
    "cosineSimilarities = cosine_similarity(query_tfidf, corpus_tfidf).flatten()\n",
    "\n",
    "print(len(cosineSimilarities))\n",
    "cosineData={}\n",
    "for c in range(0,len(cosineSimilarities)):\n",
    "    cosineData[c]=cosineSimilarities[c]\n",
    "sortedCosineData = sorted(cosineData.items(), key=lambda kv: kv[1], reverse=True)\n",
    "for s in sortedCosineData[0:10]:\n",
    "    print(s,list(token_dict)[s[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
