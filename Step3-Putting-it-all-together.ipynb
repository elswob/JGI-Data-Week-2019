{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To recap\n",
    "\n",
    "### Data so far\n",
    "\n",
    "1. We have created a list of people and their ORCID identifiers - [data/pure_person_to_orcid.txt](data/pure_person_to_orcid.txt)\n",
    "2. We have created a list of ORCID to PubMed identifiers - [data/orcid.tsv](data/orcid.tsv)\n",
    "3. We have created a list of PubMed IDs to PubMed info - [data/pubmed.tsv](data/pubmed.tsv)\n",
    "4. We have created a list of the top 100 terms for each person - [data/orcid-tf-idf.txt](data/orcid-tf-idf.txt)\n",
    "\n",
    "### Questions\n",
    "\n",
    "Now we need to use the common identifiers in these data to answer some questions:\n",
    "\n",
    "1. Can we produce a set of potential collaborators for each person, a collaborator being someone they have significant terms in common with but not previously published with.\n",
    "2. Using a piece of text, such as a conference or publication, can we select a set of people that most closely map to it?\n",
    "3. If we were to compare the publications from a person outside of our group, where would they fit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some dataframes\n",
    "\n",
    "Normally at this point we would use a database, and the phrase `knowledge graph` in the title of the workshop suggests that would be the aim here (if I were to use a database at this point, I would use Neo4j - https://neo4j.com/). However, to keep it all in a single language and framework, and as the data themselves are relatively small, we can use pandas - https://pandas.pydata.org/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, following on from the data descriptions above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "# 1. People and their ORCID identifiers \n",
    "personToOrcid = pd.read_csv(config.demoPureOrcidFile,sep='\\t')\n",
    "print(personToOrcid.head())\n",
    "print(personToOrcid.shape)\n",
    "\n",
    "# 2. ORCID to PubMed identifiers\n",
    "orcidToPubmed = pd.read_csv(config.demoOrcidFile,sep='\\t')\n",
    "print(orcidToPubmed.head())\n",
    "print(orcidToPubmed.shape)\n",
    "\n",
    "# 3. PubMed IDs to PubMed info\n",
    "pubmedToInfo = pd.read_csv(config.demoPubmedFile,sep='\\t')\n",
    "print(pubmedToInfo.head())\n",
    "print(pubmedToInfo.shape)\n",
    "\n",
    "# 4. Top 100 terms for each person\n",
    "topTerms = pd.read_csv(config.demoTfidfFile,sep='\\t')\n",
    "print(topTerms.head())\n",
    "print(topTerms.shape)\n",
    "\n",
    "# 5. PURE to name\n",
    "pureToName = pd.read_csv(config.demoPurePeopleFile,sep='\\t')\n",
    "print(pureToName.head())\n",
    "print(pureToName.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first pass would be to count the number of matching terms between each person. To start with, let's just compare the first in the list to everyone else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through people, limit to top n in list\n",
    "def compare_people(limit):\n",
    "    for i, iData in personToOrcid.head(n=limit).iterrows():\n",
    "        print('### Comparing',i,iData.pure_person_id,iData.orcid_id)\n",
    "        #get all terms and tf-idf values\n",
    "        iTopTerms=topTerms[topTerms['orcid_id']==iData.orcid_id]\n",
    "        iTerms = iTopTerms['term']\n",
    "\n",
    "        #compare to all other people\n",
    "        for j in range(i+1,personToOrcid.shape[0]):\n",
    "            jData=personToOrcid.iloc[j]\n",
    "            jTopTerms=topTerms[topTerms['orcid_id']==jData.orcid_id]\n",
    "            jTerms = jTopTerms['term']\n",
    "            com = list(set(iTerms) & set(jTerms))\n",
    "            #only show maches with more than 1 matching term\n",
    "            if len(com) > 1:\n",
    "                print(jData.orcid_id,com)\n",
    "    \n",
    "compare_people(1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few matches with many overlapping terms, except one **0000-0001-7086-8882**. Let's look at the research pages of these two people\n",
    "\n",
    "https://research-information.bristol.ac.uk/en/persons/melody-a-s-sylvestre(81e7f06c-77d8-4020-9608-0b30dd001c43)/publications.html https://research-information.bristol.ac.uk/en/persons/nicholas-a-teanby(4ec18a96-fd2e-4311-a6fa-7ec65696a4e9)/publications.html \n",
    "\n",
    "This suggests they are from similar research areas, and have indeed co-published.   \n",
    "\n",
    "Let's try again, this time with the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_people(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second person matched no-one, the third however matched quite a few. Let's modify the function to sort by number:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_people(limit):\n",
    "    for i, iData in personToOrcid.head(n=limit).iterrows():\n",
    "        print('### Comparing',i,iData.pure_person_id,iData.orcid_id)\n",
    "        #get all terms and tf-idf values\n",
    "        iTopTerms=topTerms[topTerms['orcid_id']==iData.orcid_id]\n",
    "        iTerms = iTopTerms['term']\n",
    "        jComp={}\n",
    "        for j in range(i+1,personToOrcid.shape[0]):\n",
    "            jData=personToOrcid.iloc[j]\n",
    "            jTopTerms=topTerms[topTerms['orcid_id']==jData.orcid_id]\n",
    "            jTerms = jTopTerms['term']\n",
    "            com = list(set(iTerms) & set(jTerms))\n",
    "            #only show maches with more than 1 matching term\n",
    "            if len(com) > 1:\n",
    "                jComp[jData.orcid_id]=com\n",
    "        #create sorted dictionary using number of items\n",
    "        jComp = sorted(jComp.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "        for p in jComp[0:10]:\n",
    "            print(p)\n",
    "            \n",
    "compare_people(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only shows us terms that are in common. It could be possible to have lots of terms in common, but these terms are not the 'most' representative of either person, or indeed in order of importance per person. It would be better to compare all terms, and their tf-idf values simultaneously.\n",
    "\n",
    "Back in the TF-IDF section (http://localhost:8888/lab#TF-IDF-using-sklearn) we created a similarity matrix from the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity matrix for all people\n",
    "\n",
    "matrixCom = {}\n",
    "\n",
    "#get tfidf matrix and orcidText dictionary\n",
    "%store -r matrix\n",
    "%store -r token_dict\n",
    "\n",
    "for i in range(0,len(matrix)):\n",
    "    iOrcid=list(token_dict)[i]\n",
    "    matrixCom[iOrcid]={}\n",
    "    for j in range(0,len(matrix)):\n",
    "        jOrcid=list(token_dict)[j]\n",
    "        matrixCom[iOrcid][jOrcid]=matrix[i][j]\n",
    "\n",
    "o = open(config.orcidToOrcid,'w')\n",
    "o.write('orcid_1\\torcid_2\\ttf-idf\\n')\n",
    "counter=0\n",
    "for m in matrixCom:\n",
    "    sorted_res = sorted(matrixCom[m].items(), key=lambda kv: kv[1], reverse=True)\n",
    "    if counter<3:\n",
    "        print(m)\n",
    "        for s in sorted_res[0:5]:\n",
    "            print('\\t',s[0],s[1])\n",
    "        counter+=1\n",
    "    for s in sorted_res:\n",
    "        o.write(m+'\\t'+s[0]+'\\t'+str(s[1])+'\\n')\n",
    "o.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collaboration recommendation engine\n",
    "\n",
    "This similarity data depicts the similarity between each person's publication text based on tf-idf. Often, similarities arise due to co-publication, and perhaps a more informative recommender would be to idenfity cases where people have signficant overlap in their publication text, but have never previously co-published. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the new data into a dataframe\n",
    "\n",
    "orcidToOrcid = pd.read_csv(config.demoOrcidToOrcid,sep='\\t')\n",
    "print(orcidToOrcid.shape)\n",
    "print(orcidToOrcid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter on co-publishing\n",
    "example_orcid='0000-0001-7328-4233'\n",
    "#example_orcid='0000-0003-0924-3247'\n",
    "example_pubs=orcidToPubmed[orcidToPubmed['orcid_id']==example_orcid]['pmid']\n",
    "\n",
    "def find_overlapping_tfidf(orcid1,orcid2):\n",
    "    o1=topTerms[topTerms['orcid_id']==orcid1]\n",
    "    o2=topTerms[topTerms['orcid_id']==orcid2]\n",
    "    m = o1.merge(o2, left_on='term', right_on='term')\n",
    "    return m.iloc[:, [1,2,4]]\n",
    "\n",
    "counter=0\n",
    "oColab=orcidToOrcid[orcidToOrcid['orcid_1']==example_orcid]['orcid_2']\n",
    "for o in oColab:\n",
    "    colabPubs=orcidToPubmed[orcidToPubmed['orcid_id']==o]['pmid']\n",
    "    pubCom=len(set(example_pubs).intersection(set(colabPubs)))\n",
    "    #find cases where there are no common publications\n",
    "    if pubCom==0:\n",
    "        if counter<5:\n",
    "            #get overlapping tf-idf terms\n",
    "            overlapPubs = find_overlapping_tfidf(example_orcid,o)\n",
    "            r = orcidToOrcid[(orcidToOrcid['orcid_1']==example_orcid) & (orcidToOrcid['orcid_2']==o)]\n",
    "            print(o,r['tf-idf'].to_string(header=False))\n",
    "            print(overlapPubs,'\\n')\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matching text to people\n",
    "\n",
    "Using the same tf-idf model, we can try and match any piece of text to the most relevant people. To do this we will compare the tf-idf vectors for a piece of text to each person using the cosine similarity of the vectors. For more info - https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "\n",
    ">The cosine measure similarity is another similarity metric that depends on envisioning user preferences as points in space.  Hold in mind the image of user preferences as points in an n-dimensional space. Now imagine two lines from the origin, or  point (0,0,…,0), to each of these two points. When two users are similar, they’ll have similar ratings, and so will be  relatively close in space—at least, they’ll be in roughly the same direction from the origin. The angle formed between these two lines will be relatively small. In contrast, when the two users are dissimilar, their points will be distant, and likely in different directions from the origin, forming a wide angle. This angle can be used as the basis for a similarity metric in the same way that the Euclidean distance was used to form a similarity metric. In this case, the cosine of the angle leads to a similarity value. <b>If you’re rusty on trigonometry, all you need to remember to understand this is that the cosine value is always between –1 and 1: the cosine of a small angle is near 1, and the cosine of a large angle near 180 degrees is close to –1. This is good, because small angles should map to high similarity, near 1, and large angles should map to near –1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/55677314/using-sklearn-how-do-i-calculate-the-tf-idf-cosine-similarity-between-documents\n",
    "%store -r tfs\n",
    "corpus_tfidf = tfs\n",
    "%store -r tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#merge pure people data  to get names\n",
    "pure_people_orcid = personToOrcid.merge(pureToName, left_on='pure_person_id', right_on='pure_person_id')\n",
    "\n",
    "def compare_text(query,topMatches=5):\n",
    "    print('\\n','# Comparing text:',query[:100],'...')\n",
    "    query_tfidf = tfidf.transform([query])\n",
    "    \n",
    "    #get terms for query\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    res={}\n",
    "    for col in query_tfidf.nonzero()[1]:\n",
    "        res[feature_names[col]]=query_tfidf[0, col]\n",
    "        #reverse sort the results\n",
    "        sorted_res = sorted(res.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    #compare tfidf vectors using cosine similarity\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, corpus_tfidf).flatten()\n",
    "    cosineData={}\n",
    "    \n",
    "    #order cosine similarities \n",
    "    for c in range(0,len(cosineSimilarities)):\n",
    "        cosineData[c]=cosineSimilarities[c]\n",
    "    sortedCosineData = sorted(cosineData.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    #process the output and get more info for matches\n",
    "    for s in sortedCosineData[0:topMatches]:\n",
    "        #get pureID \n",
    "        orcid=list(token_dict)[s[0]]\n",
    "        p = pure_people_orcid[personToOrcid['orcid_id']==orcid]\n",
    "        if not p.empty:\n",
    "            print('\\n',s[1],orcid,p['person_name'].values[0])\n",
    "            #get overlapping terms\n",
    "            o1=topTerms[topTerms['orcid_id']==orcid]\n",
    "            o2=sorted_res[:100]\n",
    "            #create a dataframe for results\n",
    "            data=[]\n",
    "            for i in o2:\n",
    "                if i[0] in o1.term.values:\n",
    "                    data.append([i[0], i[1], o1[o1['term']==i[0]]['tf-idf'].values[0]])\n",
    "            df=pd.DataFrame(data, columns=[\"term\", \"doc-tfidf\", \"person-tdidf\"])\n",
    "            print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PubMed article 26930047\n",
    "query=\"\"\"\n",
    "Diagnosis of Coronary Heart Diseases Using Gene Expression Profiling; Stable Coronary Artery Disease, Cardiac Ischemia with and without Myocardial Necrosis.\n",
    "Cardiovascular disease (including coronary artery disease and myocardial infarction) is one of the leading causes of death in Europe, and is influenced by both \n",
    "environmental and genetic factors. With the recent advances in genomic tools and technologies there is potential to predict and diagnose heart disease using \n",
    "molecular data from analysis of blood cells. We analyzed gene expression data from blood samples taken from normal people (n = 21), non-significant coronary artery \n",
    "disease (n = 93), patients with unstable angina (n = 16), stable coronary artery disease (n = 14) and myocardial infarction (MI; n = 207). We used a feature \n",
    "selection approach to identify a set of gene expression variables which successfully differentiate different cardiovascular diseases. The initial features \n",
    "were discovered by fitting a linear model for each probe set across all arrays of normal individuals and patients with myocardial infarction. Three different \n",
    "feature optimisation algorithms were devised which identified two discriminating sets of genes, one using MI and normal controls (total genes = 6) and another \n",
    "one using MI and unstable angina patients (total genes = 7). In all our classification approaches we used a non-parametric k-nearest neighbour (KNN) classification \n",
    "method (k = 3). The results proved the diagnostic robustness of the final feature sets in discriminating patients with myocardial infarction from healthy controls. \n",
    "Interestingly it also showed efficacy in discriminating myocardial infarction patients from patients with clinical symptoms of cardiac ischemia but no myocardial \n",
    "necrosis or stable coronary artery disease, despite the influence of batch effects and different microarray gene chips and platforms.\n",
    "\"\"\"\n",
    "compare_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "genome wide association gwas\n",
    "\"\"\"\n",
    "compare_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "quantum mechanics\n",
    "\"\"\"\n",
    "compare_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from file\n",
    "#https://www.bristol.ac.uk/cancer/events/2019/bioc-16may.html\n",
    "text=open('data/cancer-conf.txt','r').read()\n",
    "compare_text(text.lower(),topMatches=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare a person\n",
    "\n",
    "How would a person outside of this group of people compare. If you have an ORCID, or know of one, try it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.common_functions import orcid_to_pubmedData\n",
    "\n",
    "example_orcid='0000-0001-9918-058X'\n",
    "\n",
    "#get publication data fron orcid and write to file\n",
    "orcid_to_pubmedData([example_orcid])\n",
    "\n",
    "#load publication ids for orcid data\n",
    "pubData=set()\n",
    "with open(config.orcidFile,'r') as f:\n",
    "    for line in f:\n",
    "        orcid,pmid = line.rstrip().split('\\t')\n",
    "        if orcid==example_orcid:\n",
    "            pubData.add(pmid)\n",
    "#print(pubData)\n",
    "\n",
    "#load publication text matching PubMed IDs\n",
    "text=''\n",
    "with open(config.pubmedFile,'r') as f:\n",
    "    for line in f:\n",
    "        pmid,year,title,abstract = line.rstrip().split('\\t')\n",
    "        if pmid in pubData:\n",
    "            pubData.add(pmid)\n",
    "            text+=title+' '+abstract\n",
    "if len(text)>10:\n",
    "    compare_text(text.lower(),topMatches=10)\n",
    "else:\n",
    "    print('Not enough text',len(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "This is a very crude demonstration of how you might do this kind of thing. Natural language processing methods are developing at an incredible rate, and there are many alternative methods, e.g. word2vec, doc2vec, BERT. We haven't covered many important areas, such as entity tagging, context aware text processing, stemming/lematizing, building graphs, network analysis, etc. Maybe next time :)\n",
    "\n",
    "There are also lots of alternative tools, e.g. Scopus, Scival, Fingerprints (Pure) but these are mainly closed source and provide no specific information on methods. I plan to develop the above ideas in relation to AXON and welcome collaborations and ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
